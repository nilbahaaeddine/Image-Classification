{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T11:51:17.978410Z",
     "start_time": "2020-05-09T11:51:13.845326Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mnist\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Déclarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T11:51:18.185828Z",
     "start_time": "2020-05-09T11:51:17.981845Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histories (eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})\n",
    "\n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis = 0) - 0.1\n",
    "    amax, bmax = X.max(axis = 0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "\n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap = cm, alpha = 0.8)\n",
    "\n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y, cmap = cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01, x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x <= 0] = 0.01\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx / expx.sum(axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T10:59:21.198003Z",
     "start_time": "2020-05-10T10:59:13.558370Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv3x3:\n",
    "    # A Convolution layer using 3x3 filters.\n",
    "\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        print (\"Creation d'un Conv3x3 avec \", num_filters)\n",
    "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "        # We divide by 9 to reduce the variance of our initial values\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        #on doit faire ne sorte que l'entrée soit un tableau 3D non 2d\n",
    "        print (\"Je suis dans le forward de Conv3x3\")\n",
    "        self.last_input = input\n",
    "        print(input.shape)\n",
    "        #h, w = input.shape\n",
    "        h=input.shape[0]\n",
    "        w=input.shape[1]\n",
    "        output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "          for f in range(self.num_filters):\n",
    "            d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "        # Update filters\n",
    "        self.filters -= learn_rate * d_L_d_filters\n",
    "\n",
    "        # We aren't returning anything here since we use Conv3x3 as the first layer in our CNN.\n",
    "        # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
    "        # other layer in our CNN.\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "class MaxPool2:\n",
    "    # A Max Pooling layer using a pool size of 2.\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        print (\"je suis dans le forward de MaxPool2\")\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "          h, w, f = im_region.shape\n",
    "          amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "          for i2 in range(h):\n",
    "            for j2 in range(w):\n",
    "              for f2 in range(f):\n",
    "                # If this pixel was the max value, copy the gradient to it.\n",
    "                if im_region[i2, j2, f2] == amax[f2]:\n",
    "                  d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input\n",
    "    \n",
    "class MyFlatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        print(f'input : {input.shape}')\n",
    "        print(f'output : {input.flatten().shape}')\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        return input\n",
    "    def backprop(self, d_L_d_out):\n",
    "        return d_L_d_out.reshape(self.last_input_shape)\n",
    "    \n",
    "class MyLayer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.input = kwargs.get(\"input\", None) # Number of neurons at layer i-1\n",
    "        self.output = kwargs.get(\"output\", None) # Number of neurons at  layer i (current layer) \n",
    "        self.activ_function_curr = kwargs.get(\"activation\", None) # Activation function for the layer\n",
    "        self.paramCouche = kwargs.get(\"paramCouche\", None) # Param de la couche Conv3x3\n",
    "        self.type = kwargs.get(\"type\", None)\n",
    "        self.couche = kwargs.get(\"couche\", None)\n",
    "        self.parameters = {}\n",
    "        self.derivatives = {}\n",
    "        self.activation_func = None\n",
    "        #self.activationCNNFunc = None\n",
    "        #self.outputCNN = None\n",
    "        \n",
    "        \n",
    "        if self.activ_function_curr == \"relu\":\n",
    "                self.activation_func = relu\n",
    "                self.backward_activation_func = relu_prime\n",
    "        elif self.activ_function_curr == \"sigmoid\":\n",
    "                self.activation_func = sigmoid\n",
    "                self.backward_activation_func = sigmoid_prime\n",
    "        elif self.activ_function_curr == \"tanh\":\n",
    "                self.activation_func = tanh\n",
    "                self.backward_activation_func = tanh_prime\n",
    "        elif self.activ_function_curr == \"leakyrelu\":\n",
    "                self.activation_func = leakyrelu\n",
    "                self.backward_activation_func = leakyrelu_prime\n",
    "        elif self.activ_function_curr == \"softmax\":\n",
    "                self.activation_func = softmax\n",
    "                self.backward_activation_func = softmax\n",
    "        \n",
    "              \n",
    "                \n",
    "                \n",
    "    def initParams(self):\n",
    "        # Initialisation du dictionnaire de données parameters contenant W, A et Z pour un layer\n",
    "        print (\"Je suis dans init Params, là je mets des W et b avec des randoms\")\n",
    "        seed = 30\n",
    "        np.random.seed(seed)\n",
    "        self.parameters['W'] = np.random.randn(self.output, self.input) * np.sqrt(2 / self.input)\n",
    "        self.parameters['b'] = np.random.randn(self.output, 1) * 0.1\n",
    "\n",
    "    def setW(self, matW):\n",
    "        self.parameters['W'] = np.copy(matW)\n",
    "        \n",
    "    def setA(self, matA):\n",
    "        self.parameters['A'] = np.copy(matA) \n",
    "        \n",
    "    def setZ(self, matZ):\n",
    "        self.parameters['Z'] = np.copy(matZ)\n",
    "    \n",
    "    def setB(self, matB):\n",
    "        self.parameters['b'] = np.copy(matB)\n",
    "        \n",
    "    def setdW(self, matdW):\n",
    "        self.parameters['dW'] = np.copy(matdW)\n",
    "        \n",
    "    def setdA(self, matdA):\n",
    "        self.parameters['dA'] = np.copy(matdA)\n",
    "        \n",
    "    def setdZ(self, matdZ):\n",
    "        self.parameters['dZ'] = np.copy(matdZ)\n",
    "    \n",
    "    def setdB(self, matdB):\n",
    "        self.parameters['db'] = np.copy(matdB)\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.nbCNNlayers = 0\n",
    "        self.layers = [] # NN layers\n",
    "        #self.CNN = [] # CNN layers\n",
    "        self.CNN_EXIST=False\n",
    "        self.outFlat = 0\n",
    "\n",
    "    def printLayers(self):\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(self.CNN[i].activationCNNFunc)\n",
    "        for i in range(len(self.layers)):\n",
    "            print(self.layers[i].activation_func)\n",
    "        \n",
    "    def info(self):\n",
    "        print(f'Content of the network :');\n",
    "        j = 0;\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(f'\\n\\tLayer n° {i} du CNN => ')\n",
    "            print(f'\\t\\tInput : {self.CNN[i].input}\\n\\t\\tOutput : {self.CNN[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.CNN[i].activationCNNFunc}')\n",
    "                print(f'\\t\\tW shape : {self.CNN[i].parameters[\"W\"].shape}\\n')\n",
    "                #t\\tW data :\\n{self.CNN[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.CNN[i].parameters[\"b\"].shape}\\n')\n",
    "                #\\t\\tb data :\\n{self.CNN[i].parameters[\"b\"]}')\n",
    "                \n",
    "        for i in range(len(self.layers)):\n",
    "            print(f'\\n\\tLayer n° {i} du NN => ')\n",
    "            print(f'\\t\\tInput : {self.layers[i].input}\\n\\t\\tOutput : {self.layers[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "                print(f'\\t\\tW shape : {self.layers[i].parameters[\"W\"].shape}\\n')\n",
    "                #\\t\\tW data :\\n{self.layers[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.layers[i].parameters[\"b\"].shape}\\n')\n",
    "                #\\t\\tb data :\\n{self.layers[i].parameters[\"b\"]}')\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        self.nbLayers += 1 # Le layer 0 = input, si CNN = images\n",
    "        print (\"Layer\",layer)\n",
    "        if (type(layer) is Conv3x3):\n",
    "            print (\"c'est un 3x3\")\n",
    "            self.layers.append(layer)\n",
    "            self.CNN_EXIST=True # il existe un CNN donc pour le RNN il faudra prendre la sortie du flatten\n",
    "        if (type(layer) is MaxPool2):    \n",
    "            print (\"c'est un maxPool2\")\n",
    "            self.layers.append(layer)\n",
    "        if (type(layer) is MyFlatten):    \n",
    "            print (\"c'est un Flatten\")\n",
    "            self.layers.append(layer)\n",
    "        if (type(layer) is MyLayer):    \n",
    "            print (\"c'est un MyLayer\") \n",
    "            self.layers.append(layer)           \n",
    "            # attention s'il y a un CNN on n'a pas initialisé les valeurs\n",
    "            # il faut le faire dans le predict avec un init_Params()\n",
    "            if self.CNN_EXIST != True: # on est dans le cas normal où il n'y a pas de CNN\n",
    "                if (self.nbLayers==1): \n",
    "                    # this is the first layer so adding a layer 0\n",
    "                    layerZero=Layer(layer.input)\n",
    "                    self.layers.append(layerZero)\n",
    "            \n",
    "                self.layers.append(layer) \n",
    "                self.layers[self.nbLayers].input=self.layers[self.nbLayers-1].output\n",
    "                #self.layers[self.nbLayers].output=self.layers[self.nbLayers].output\n",
    "                layer.initParams()\n",
    "           #si on a un CNN, il faut récupérer la valeur du flatten pour le forward\n",
    "                    \n",
    "          \n",
    "\n",
    "    def set_parametersW_b (self, numlayer, matX, matb):\n",
    "        self.layers[numlayer].parameters['W'] = np.copy(matX)\n",
    "        self.layers[numlayer].parameters['b'] = np.copy(matb)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        outConv=0\n",
    "        outMaxPool=0\n",
    "        outFlatten=0\n",
    "        previousFlatten=False\n",
    "        for num_layer in range (0, self.nbLayers):\n",
    "            print (\"\\nPredict: lecture objet\",self.layers[num_layer])\n",
    "            if (type(self.layers[num_layer]) is Conv3x3):\n",
    "                outConv=self.layers[num_layer].forward(X)\n",
    "                print (\"\\tpasse dans Conv3x3\")\n",
    "            if (type(self.layers[num_layer]) is MaxPool2):\n",
    "                outMaxPool=self.layers[num_layer].forward(outConv)\n",
    "                print (\"\\tpasse dans MaxPool2\")\n",
    "            if (type(self.layers[num_layer]) is MyFlatten):\n",
    "                outFlatten=self.layers[num_layer].forward(outMaxPool) \n",
    "                self.outFlat = outFlatten\n",
    "                print (\"\\tpasse dans Flatten\")\n",
    "                previousFlatten=True\n",
    "            if (type(self.layers[num_layer]) is MyLayer):\n",
    "                # je viens de passer dans le flatten,récupération de l'input\n",
    "                if (previousFlatten): # le précédent est flatten\n",
    "                    print (\"len Flatten\", len(outFlatten))\n",
    "                    #self.layers[num_layer].input = self.layers[self.nbLayers - 1].output\n",
    "                    #layer.initParams()\n",
    "                    #il faut initialiser les valeurs des paramètres\n",
    "                    print (\"Init des paramètres. Output = \",self.layers[num_layer].output)\n",
    "                    print (\"Le input doit prendre le flatten\")\n",
    "                    self.layers[num_layer].input=len(outFlatten)\n",
    "                    print (\"On relance init Param pour créer les W et b\")\n",
    "                    self.layers[num_layer].initParams()\n",
    "                    self.layers[num_layer].setZ(np.dot(self.layers[num_layer].parameters['W'], \n",
    "                                       outFlatten) + self.layers[num_layer].parameters['b'])\n",
    "                    self.layers[num_layer].setA(self.layers[num_layer].activation_func(self.layers[num_layer].parameters['Z']))\n",
    "                    print(\"self.nbLayers - activation\",self.layers[num_layer].activation_func)\n",
    "                    previousFlatten=False\n",
    "                else: # là le précédent n'est plus flatten je suis dans le RNN\n",
    "                    print (\"ICI je suis dans un RNN qui n'est pas après un flatten\")\n",
    "                    # il faut initialiser les valeurs de W et b\n",
    "                    print (\"Init des paramètres. Output = \",self.layers[num_layer].output)\n",
    "                    print (\"Le input doit prendre le layer precedent\",self.layers[num_layer-1].output)\n",
    "                    self.layers[num_layer].input=self.layers[num_layer-1].output\n",
    "                    self.layers[num_layer].initParams()\n",
    "                    self.layers[num_layer].setZ(np.dot(self.layers[num_layer].parameters['W'], \n",
    "                                       self.layers[num_layer - 1].parameters['A']) + self.layers[num_layer].parameters['b'])\n",
    "                    # Applying the activation function of the layer to Z\n",
    "                    self.layers[num_layer].setA(self.layers[num_layer].activation_func(self.layers[num_layer].parameters['Z']))\n",
    "                    #self.activation_func = softmax\n",
    "                    print(\"self.nbLayers - activation\",self.layers[num_layer].activation_func)\n",
    "                    print (\"Taille A \",self.layers[num_layer].parameters['A'].shape)\n",
    "        \n",
    "        \n",
    "    def cost_function(self, y):\n",
    "        print(\"cost_function\")\n",
    "        return (-(y * np.log(self.layers[self.nbLayers-1].parameters['A'] + 1e-8) + (1 - y) * np.log(1 - self.layers[self.nbLayers-1].parameters['A'] + 1e-8))).mean()\n",
    "\n",
    "    def backward_propagation(self, y):\n",
    "        outConv=0\n",
    "        outMaxPool=0\n",
    "        outFlatten=0\n",
    "        suivantFlatten=False\n",
    "        if(type(self.layers[self.nbLayers-1]) is MyLayer):\n",
    "            #calcul de dZ dW et db pour le dernier layer\n",
    "            print(\"self.layers[self.nbLayers].parameters['A']\",self.layers[self.nbLayers-1].parameters['A'])\n",
    "            print(\"y\",y)\n",
    "            self.layers[self.nbLayers-1].derivatives['dZ']=self.layers[self.nbLayers-1].parameters['A']-y\n",
    "            self.layers[self.nbLayers-1].derivatives['dW']=np.dot(self.layers[self.nbLayers-1].derivatives['dZ'],\n",
    "                                                                 (self.layers[self.nbLayers-1].parameters['A']))\n",
    "            m=self.layers[self.nbLayers-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "            self.layers[self.nbLayers-1].derivatives['db']=np.sum(self.layers[self.nbLayers-1].derivatives['dZ'], \n",
    "                                                           axis=1, keepdims=True) / m\n",
    "\n",
    "        #calcul de dZ dW db pour les autres layers\n",
    "        for l in range(self.nbLayers-2,0,-1) :\n",
    "            if(type(self.layers[l]) is MyLayer):\n",
    "                if(type(self.layers[l-1]) is MyFlatten):\n",
    "                    self.layers[l].derivatives['dZ']=np.dot(np.transpose(self.layers[l+1].parameters['W']),\n",
    "                                                    self.layers[l+1].derivatives['dZ'])*self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "\n",
    "                    self.layers[l].derivatives[\"dW\"]=np.dot(self.layers[l].derivatives['dZ'],self.outFlat)\n",
    "\n",
    "                    m=self.layers[l-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "                    self.layers[l].derivatives['db']=np.sum(self.layers[l].derivatives['dZ'], \n",
    "                                                               axis=1, keepdims=True) / m  \n",
    "\n",
    "\n",
    "                    outputRNN=self.layers[l].parameters['A']\n",
    "                    print(\"self.layers[l].parameters['A']\",self.layers[l].parameters['A'])\n",
    "                    suivantFlatten = True\n",
    "                else :\n",
    "                    elf.layers[l].derivatives['dZ']=np.dot(np.transpose(self.layers[l+1].parameters['W']),\n",
    "                                                    self.layers[l+1].derivatives['dZ'])*self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "\n",
    "                    self.layers[l].derivatives[\"dW\"]=np.dot(self.layers[l].derivatives['dZ'],\n",
    "                                                    (self.layers[l-1].parameters['A']))\n",
    "\n",
    "                    m=self.layers[l-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "                    self.layers[l].derivatives['db']=np.sum(self.layers[l].derivatives['dZ'], \n",
    "                                                               axis=1, keepdims=True) / m\n",
    "            if(type(self.layers[l-1]) is MyFlatten):\n",
    "                outFlatten=self.layers[l].backprop(outputRNN)\n",
    "                suivantFlatten = False\n",
    "            if(type(self.layers[l-1]) is MaxPool2):\n",
    "                outMaxPool=self.layers[l].backprop(outFlatten)\n",
    "            if(type(self.layers[l-1]) is Conv3x3):\n",
    "                outConv=self.layers[l].backprop(outMaxPool)\n",
    "        \n",
    "                      \n",
    "    def update_parameters(self, eta) :\n",
    "        for l in range(1,self.nbLayers+1) :\n",
    "            self.layers[l].parameters['W']-=eta*self.layers[l].derivatives['dW']\n",
    "            self.layers[l].parameters[\"b\"]-=eta*self.layers[l].derivatives[\"db\"]\n",
    "\n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "    def plot_W_b_epoch (self,epoch,parameter_history):\n",
    "        mat=[]\n",
    "        max_size_layer=0\n",
    "        for l in range(1, self.nbLayers+1):    \n",
    "            value=parameter_history[epoch]['W'+str(l)]\n",
    "            if (parameter_history[epoch]['W'+str(l)].shape[1]>max_size_layer):\n",
    "                max_size_layer=parameter_history[epoch]['W'+str(l)].shape[1]\n",
    "            mat.append(value)\n",
    "        figure=plt.figure(figsize=((self.nbLayers+1)*3,int (max_size_layer/2)))    \n",
    "        for nb_w in range (len(mat)):    \n",
    "                plt.subplot(1, len(mat), nb_w+1)\n",
    "                plt.matshow(mat[nb_w],cmap = plt.cm.gist_rainbow,fignum=False, aspect='auto')\n",
    "                plt.colorbar()    \n",
    "        thelegend=\"Epoch \"+str(epoch)\n",
    "        plt.title (thelegend)    \n",
    "\n",
    "    def accuracy(self,y_hat, y):\n",
    "        if self.layers[self.nbLayers-1].activation_func==softmax:\n",
    "            # si la fonction est softmax, les valeurs sont sur différentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # où est la valeur maximale à la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded=np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis=0)\n",
    "            y_encoded=np.copy(y)\n",
    "            y_encoded=np.argmax(y_encoded, axis=0)\n",
    "            return (y_hat_encoded == y_encoded).mean()\n",
    "        # la dernière fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilité du résultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()       \n",
    "\n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.layers[self.nbLayers-1].parameters['A']\n",
    "\n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        print (\"m ds next_batch\",m)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        batchsize=kwargs.get(\"batchsize\",32)\n",
    "        print (\"Dans fit, X.shape, y.shape\",X.shape,len(X),y.shape,y[0:2])\n",
    "        #def fit(self, X, y, epochs, eta = 0.01,batchsize=64) :\n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        parameter_history = []\n",
    "        for i in range(epochs):\n",
    "            i+=1\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost = []\n",
    "            accuracy = []\n",
    "            ##########\n",
    "            # on ne va passer qu'une image à la fois\n",
    "            for nb_x in range(y.shape[0]):\n",
    "                self.forward_propagation(X[nb_x])\n",
    "                self.backward_propagation(y[nb_x])\n",
    "                #self.update_parameters(eta)\n",
    "                #out=self.layers[self.nbLayers-1].parameters['A']\n",
    "                #loss = -np.log(out[y[nb_x]])\n",
    "                #acc = 1 if np.argmax(out) == y[nb_x] else 0\n",
    "                print (nb_x,\" calcul du cout pour \")\n",
    "                print (y[nb_x])\n",
    "                current_cost=self.cost_function(y[nb_x])\n",
    "                print (\"\\t\\tcurrent_cost\",current_cost)\n",
    "                \n",
    "                cost.append(current_cost)\n",
    "                y_hat = self.predict(X[nb_x])\n",
    "                current_accuracy = self.accuracy(y_hat,y[nb_x])\n",
    "                print (\"\\t\\tPour y[\",nb_x,\"]\",y[nb_x],\" valeur prédite\",y_hat)\n",
    "                \n",
    "                ############## TODO ###########\n",
    "                # Revoir si on récupère bien l'accuracy \n",
    "                # Faire la backpropagation + update parameters\n",
    "                    # Extraction et traitement d'un batch à la fois\n",
    "                '''\n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                if self.layers[self.nbLayers-1].activation_func==softmax:\n",
    "                    # la classification n'est pas binaire, y a utilisé one-hot-encoder\n",
    "                    # le batchy doit donc être transposé et le résultat doit\n",
    "                    # être sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                    print(\"self.layers[self.nbLayers].activation_func==softmax\")\n",
    "                    print(\"batchy.shape[0]\",batchy.shape[0],batchy.shape[1])\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe pas\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                    print(\"else self.layers[self.nbLayers].activation_func==softmax\")\n",
    "                #batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                print (\"batchy\",batchy)\n",
    "                self.forward_propagation(batchX)\n",
    "                #self.backward_propagation(batchy)\n",
    "                #self.update_parameters(eta)\n",
    "                '''\n",
    "                      \n",
    "                # sauvegarde pour affichage\n",
    "\n",
    "                \n",
    "                accuracy.append(current_accuracy)\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            ##############\n",
    "            \n",
    "                \n",
    "            #parameter_history.append(save_values)        \n",
    "            #sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(verbose == True):\n",
    "                print(f'Epoch : {i}/{epochs} | cost : {float(current_cost)}| accuracy : {current_accuracy}')\n",
    "              \n",
    "        return self.layers, cost_history, accuracy_history, parameter_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de mnist </h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atemp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T10:59:21.893627Z",
     "start_time": "2020-05-10T10:59:21.201060Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = mnist.test_images()[:10]\n",
    "test_labels = mnist.test_labels()[:10]\n",
    "\n",
    "\n",
    "# mis au bon format des images\n",
    "newTestImages = []\n",
    "for image in test_images:\n",
    "    newTestImages.append(((image / 255) - 0.5))\n",
    "\n",
    "X = np.float64(newTestImages)\n",
    "\n",
    "#transformation des y en tableau binaire\n",
    "y = to_categorical(test_labels, num_classes=10)   \n",
    "print (\"transformation des y pour etre au bon format\", y.shape)\n",
    "print (\"exemple des donnees de y\",y[0:3])\n",
    "\n",
    "# Jeu d'apprentissage 60%\n",
    "validation_size = 0.5\n",
    "\n",
    "# 40% du jeu de données pour le test\n",
    "testsize = 1 - validation_size\n",
    "\n",
    "seed = 30\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = validation_size, random_state = seed, test_size = testsize)\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prédictives)\n",
    "#X_train=np.transpose(X_train)\n",
    "print (\"X_train.shape, y_train.shape\", X_train.shape,y_train.shape)\n",
    "# y_train est forcé pour être un tableau à 1 ligne contenant m colonnes\n",
    "#y_train=np.transpose(y_train.reshape((y_train.shape[0], y_train.shape[1])))\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "#X_test=np.transpose(X_test)\n",
    "#y_test=np.transpose(y_test.reshape((y_test.shape[0], y_test.shape[1])))\n",
    "\n",
    "# ATTENTION DANS LE MODELE COMME ON PASSE UNE IMAGE A LA FOIS IL FAUT QUE LE OUTPUT soit à 1\n",
    "# cf ci-dessous\n",
    "network = MyNeuralNetwork()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(MyLayer(output = 1, activation = \"relu\"))\n",
    "network.addLayer(MyLayer(output = 10, activation = \"softmax\"))\n",
    "\n",
    "# le network info ne fonctionne pas je ne l'ai pas mis à jour\n",
    "#network.info()\n",
    "\n",
    "epochs = 5\n",
    "eta = 0.01\n",
    "batchsize=20\n",
    "\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "print(f'shape de X_train')\n",
    "print(X_train.shape)\n",
    "print(f'shape de X_test')\n",
    "print(X_test.shape)\n",
    "print(f'shape de y_train')\n",
    "print(y_train.shape)\n",
    "print (y_train)\n",
    "print(f'shape de y_test')\n",
    "print(y_test.shape)\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "\n",
    "#Entraînement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "##### Attention les fonctions en dessous ne marcheront pas car je n'ai pas sauvegardé l'historique\n",
    "# pour la prédiction il faut s'inspirer de y_hat = self.predict(X[nb_x])\n",
    "# pour l'instant predict attend un tableau, là il faut passer valeur par valeur\n",
    "#Prédiction\n",
    "for i in range (y_test.shape[0]):\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    accuracy_test = network.accuracy(y_pred, y_test[i])\n",
    "    print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "#y_pred=network.predict(X_test)\n",
    "#accuracy_test = network.accuracy(y_pred, y_test)\n",
    "#print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "\n",
    "# Affichage des historiques\n",
    "#plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
