{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:40:12.258597Z",
     "start_time": "2020-05-11T13:40:12.245490Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "import time\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Déclarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T12:43:00.213358Z",
     "start_time": "2020-05-11T12:42:59.967458Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histories (eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3: # Une couche e convolution qui utilise un filtre de 3X3\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # Les filtres dont des tableaux 3D de dimensions(num_filters, 3, 3)\n",
    "        # On divise par 9 pour reduire la variance de notre valeur initiale\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j                \n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        if(input.ndim == 2): # 1er conv on recup l'image 2D\n",
    "            self.last_input = input\n",
    "            h, w = input.shape\n",
    "            output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "            for im_region, i, j in self.iterate_regions(input):\n",
    "                output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        else: # 2ème conv on récup le resultat du maxpool 3D\n",
    "            self.last_input = input\n",
    "            h, w , filters = input.shape\n",
    "            output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "            temp=np.transpose(input)\n",
    "            for k in range(0, filters):\n",
    "                for im_region, i, j in self.iterate_regions(temp[k]):\n",
    "                    output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "                    \n",
    "        return output\n",
    "    \n",
    "    def iterate_regions_back(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        if(self.last_input.ndim == 2): # 1er conv on recup l'image 2D\n",
    "            d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "            for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "                for f in range(self.num_filters):\n",
    "                    d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "            # Update filters\n",
    "            self.filters -= learn_rate * d_L_d_filters\n",
    "            # We aren't returning anything here since we use Conv3x3 as\n",
    "            # the first layer in our CNN. Otherwise, we'd need to return\n",
    "            # the loss gradient for this layer's inputs, just like every\n",
    "            # other layer in our CNN.\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "            for im_region, i, j in self.iterate_regions_back(self.last_input):\n",
    "                h, w, f = im_region.shape\n",
    "                amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "                for i2 in range(h):\n",
    "                    for j2 in range(w):\n",
    "                        for f2 in range(f):\n",
    "                            # If this pixel was the max value, copy the gradient to it.\n",
    "                            if im_region[i2, j2, f2] == amax[f2]:\n",
    "                                d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "            return d_L_d_input\n",
    "\n",
    "        \n",
    "class Relu:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        self.last_input = input\n",
    "        output = np.maximum(0, input)  # element-wise\n",
    "        \n",
    "        #print('Forward relu')\n",
    "        return output\n",
    "\n",
    "    def backprop(self, dOut):\n",
    "        \"\"\"\n",
    "        Backward propogation.\n",
    "        f′(x) = {1 if x > 0}\n",
    "                {0 otherwise}\n",
    "        \"\"\"\n",
    "        dOut[dOut <= 0] = 0\n",
    "        dOut[dOut > 0] = 1\n",
    "        \n",
    "        return dOut\n",
    "    \n",
    "class MaxPool2: # A Max Pooling layer using a pool size of 2.\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        # If this pixel was the max value, copy the gradient to it.\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, drop_probability):\n",
    "        self.drop_probability = drop_probability\n",
    "    \n",
    "    def forward(self,input):\n",
    "        keep_probability = 1 - self.drop_probability\n",
    "        mask = nd.random_uniform(0, 1.0, input.shape) < keep_probability\n",
    "        mask=mask.asnumpy()\n",
    "        #############################\n",
    "        #  Avoid division by 0 when scaling\n",
    "        #############################\n",
    "        if keep_probability > 0.0:\n",
    "            scale = (1/keep_probability)\n",
    "        else:\n",
    "            scale = 0.0\n",
    "        out=input*mask*scale\n",
    "        return out\n",
    "\n",
    "class MyFlatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        return input\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        return d_L_d_out.reshape(self.last_input_shape)\n",
    "\n",
    "class Dense: # A standard fully-connected layer with softmax activation.\n",
    "    def __init__(self, input_len, nodes,**kwargs):\n",
    "        self.activ_function_curr = kwargs.get(\"activation\",None)\n",
    "        self.activation_func=None\n",
    "        # We divide by input_len to reduce the variance of our initial values\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the softmax layer using the given input.\n",
    "        Returns a 1d numpy array containing the respective probability values.\n",
    "        - input can be any array with any dimensions.\n",
    "        '''\n",
    "        self.last_input_shape = input.shape\n",
    "\n",
    "        #input = input.flatten()\n",
    "        self.last_input = input\n",
    "\n",
    "        input_len, nodes = self.weights.shape\n",
    "\n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "\n",
    "        exp = np.exp(totals)\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the softmax layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float\n",
    "        '''\n",
    "        # We know only 1 element of d_L_d_out will be nonzero\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^totals\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "\n",
    "            # Sum of all e^totals\n",
    "            S = np.sum(t_exp)\n",
    "\n",
    "            # Gradients of out[i] against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.last_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "\n",
    "            # Gradients of loss against totals\n",
    "            d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "            # Gradients of loss against weights/biases/input\n",
    "            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_t_d_b\n",
    "            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "            # Update weights / biases\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.biases -= learn_rate * d_L_d_b\n",
    "            return d_L_d_inputs\n",
    "\n",
    "\n",
    "class MyCNN:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.layers = [] # NN layers\n",
    "\n",
    "    def printLayers(self):\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(self.CNN[i].activationCNNFunc)\n",
    "        for i in range(len(self.layers)):\n",
    "            print(self.layers[i].activation_func)\n",
    "        \n",
    "    def info(self):\n",
    "        print(f'Content of the network :');\n",
    "        j = 0;\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(f'\\n\\tLayer n° {i} du CNN => ')\n",
    "            print(f'\\t\\tInput : {self.CNN[i].input}\\n\\t\\tOutput : {self.CNN[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.CNN[i].activationCNNFunc}')\n",
    "                print(f'\\t\\tW shape : {self.CNN[i].parameters[\"W\"].shape}\\n')\n",
    "                print(f'\\t\\tb shape : {self.CNN[i].parameters[\"b\"].shape}\\n')\n",
    "                \n",
    "        for i in range(len(self.layers)):\n",
    "            print(f'\\n\\tLayer n° {i} du NN => ')\n",
    "            print(f'\\t\\tInput : {self.layers[i].input}\\n\\t\\tOutput : {self.layers[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "                print(f'\\t\\tW shape : {self.layers[i].parameters[\"W\"].shape}\\n')\n",
    "                print(f'\\t\\tb shape : {self.layers[i].parameters[\"b\"].shape}\\n')\n",
    "        \n",
    "    def addLayer(self, layer):\n",
    "        self.nbLayers += 1 # Le layer 0 = input, si CNN = images\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "     \n",
    "    def forward_propagation(self, X):\n",
    "        outPrevious=0\n",
    "        for num_layer in range (0, self.nbLayers):\n",
    "            if (type(self.layers[num_layer]) is Conv3x3):\n",
    "                if num_layer==0:\n",
    "                    X=(X / 255) - 0.5\n",
    "                    outPrevious=self.layers[num_layer].forward(X)\n",
    "                else : \n",
    "                    outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "            \n",
    "            if (type(self.layers[num_layer]) is Relu):\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "                      \n",
    "            if (type(self.layers[num_layer]) is MaxPool2):\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious) \n",
    "              \n",
    "            if (type(self.layers[num_layer]) is Dropout):\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "                  \n",
    "            if (type(self.layers[num_layer]) is MyFlatten):\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious) \n",
    "                \n",
    "            if (type(self.layers[num_layer]) is Dense):\n",
    "                \n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "        return outPrevious\n",
    "               \n",
    "        \n",
    "        \n",
    "    def cost_function(self, out,y):\n",
    "        return (-np.log(out[y]))\n",
    "\n",
    "    def backward_propagation(self, out,y,eta):\n",
    "        previousGradient=0\n",
    "        for num_layer in range (self.nbLayers-1,0,-1):\n",
    "            if (type(self.layers[num_layer]) is Dense):\n",
    "                # initialisation du gradient à la fin\n",
    "                gradient = np.zeros(10)\n",
    "                gradient[y] = -1 / out[y]\n",
    "                previousGradient = self.layers[num_layer].backprop(gradient, eta)\n",
    "                #print (\"backpropagation - softmax\", previousGradient.shape)\n",
    "            \n",
    "            if (type(self.layers[num_layer]) is MyFlatten):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "                #print (\"backpropagation - flat\", previousGradient.shape) \n",
    "                      \n",
    "            if (type(self.layers[num_layer]) is MaxPool2):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "                #print (\"backpropagation - pool\", previousGradient.shape) \n",
    "            if (type(self.layers[num_layer]) is Relu):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "                #print (\"backpropagation - relu\", previousGradient.shape) \n",
    "            if (type(self.layers[num_layer]) is Conv3x3):\n",
    "                #print (\"backpropagation - Conv 3x3\")        \n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient, eta)\n",
    " \n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "\n",
    "\n",
    "    def accuracy(self,out, y):\n",
    "        acc = 1 if np.argmax(out) == y else 0\n",
    "        return acc       \n",
    "\n",
    "    def predict(self, X):\n",
    "        outPrevious=self.forward_propagation(X)\n",
    "        return outPrevious\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        timo = []\n",
    "        t = np.zeros(epochs)\n",
    "        for nb_epochs in range(epochs):\n",
    "            t[nb_epochs]=time.time()\n",
    "            if nb_epochs > 0:\n",
    "                duration = t[nb_epochs]- t[nb_epochs - 1]\n",
    "            else:\n",
    "                duration = 0\n",
    "            timo.append(duration)\n",
    "            cost_all_images=[]\n",
    "            acc_all_images=[]\n",
    "            cost = 0\n",
    "            acc = 0\n",
    "            print (\"run epoch: \",nb_epochs+1)\n",
    "            for i, (im, label) in enumerate(zip(X, y)):\n",
    "                # Do a forward pass.\n",
    "                startTime = time.time()\n",
    "                out=self.forward_propagation(im)\n",
    "                cost+=self.cost_function(out,label)\n",
    "                cost_all_images.append(self.cost_function(out,label))\n",
    "                acc+=self.accuracy(out, label)\n",
    "                acc_all_images.append(self.accuracy(out, label))\n",
    "                self.backward_propagation(out,label,eta)\n",
    "                endTime = time.time()\n",
    "                if verbose:\n",
    "                    if i % 100 == 99:\n",
    "                      print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%  ' %(i + 1, cost / 100, acc))\n",
    "                      #print(f'Step \\t{i + 1} | Past 100 steps: Average Loss :{cost / 100:.2f} |Accuracy: {acc} %| Time : {endTime - startTime:.2f}\\n')\n",
    "                      cost = 0\n",
    "                      acc = 0\n",
    "            current_cost=np.average(cost_all_images)\n",
    "            cost_history.append(current_cost)  \n",
    "            current_acc=np.average(acc_all_images)\n",
    "            accuracy_history.append(current_acc)\n",
    "            #timo.append(duration)\n",
    "        \n",
    "        return cost_history, accuracy_history\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de mnist </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:44:26.935045Z",
     "start_time": "2020-05-11T13:42:00.266679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "eta = 0.01\n",
    "num_classes = 10\n",
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# prendre moins d'images\n",
    "X_train=X_train[0:2000]\n",
    "y_train=y_train[0:2000]\n",
    "X_test=X_test[0:800]\n",
    "y_test=y_test[0:800]\n",
    "\n",
    "print (\"Learning on:\", X_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",X_test.shape[0],\" examples\")\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "#CONSTRUCTION DU RESEAU\n",
    "network = MyCNN()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(Relu())\n",
    "network.addLayer(MaxPool2()) \n",
    "network.addLayer(Dropout(0.1))\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(Dense(13 * 13 * 8, num_classes,activation=\"softmax\"))\n",
    "\n",
    "cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "accuracy_test=[]\n",
    "timo = []\n",
    "t = np.zeros(len(X_test))\n",
    "\n",
    "for i in range (len(X_test)):\n",
    "    t[i] = time.time()\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "\n",
    "    \n",
    "print(\"Accuracy test: %.3f\"%np.average(accuracy_test))\n",
    "print(\"time test:\",t[len(X_test)-1]-t[0])\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de fashion mnist </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:45:05.389271Z",
     "start_time": "2020-05-11T13:44:33.985697Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "eta = 0.01\n",
    "((X_train, y_train), (X_test, y_test)) = fashion_mnist.load_data()\n",
    "\n",
    "# prendre moins d'images\n",
    "X_train=X_train[0:2000]\n",
    "y_train=y_train[0:2000]\n",
    "X_test=X_test[0:800]\n",
    "y_test=y_test[0:800]\n",
    "\n",
    "print (\"Learning on:\", X_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",X_test.shape[0],\" examples\")\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "network = MyCNN()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(Relu())\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(Dropout(0.1))\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(Dense(13 * 13 * 8, 10,activation=\"softmax\"))\n",
    "cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "t = np.zeros(len(X_test))\n",
    "accuracy_test=[]\n",
    "for i in range (len(X_test)):\n",
    "    t[i] = time.time()\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "    \n",
    "print(\"Accuracy test: %.3f\"%np.average(accuracy_test))\n",
    "print(\"time test:\",t[len(X_test)-1]-t[0])\n",
    "\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de Cifar 10 </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "eta = 0.01\n",
    "num_classes = 10\n",
    "# The data, split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# prendre moins d'images\n",
    "X_train=X_train[0:2000]\n",
    "y_train=y_train[0:2000]\n",
    "X_test=X_test[0:800]\n",
    "y_test=y_test[0:800]\n",
    "print (\"Learning on:\", x_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",x_test.shape[0],\" examples\")\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "network = MyCNN()\n",
    "network.addLayer(Conv3x3(16))# (32,32,16)\n",
    "network.addLayer(Relu())     # (32,32,16)\n",
    "network.addLayer(MaxPool2())  # (16,16,8)\n",
    "network.addLayer(Dropout(0.1)) # (16,16,8)\n",
    "network.addLayer(MyFlatten())  # (2048)\n",
    "network.addLayer(Dense(3600, 10,activation=\"softmax\"))\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "accuracy_test=[]\n",
    "t = np.zeros(len(X_test))\n",
    "for i in range (len(X_test)):\n",
    "    t[i] = time.time()\n",
    "\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "    \n",
    "print(\"Accuracy test: %.3f\"%np.average(accuracy_test))\n",
    "print(\"time test:\",t[len(X_test)-1]-t[0])\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Data Generator a REVOIR SI ON N'A LE TEMPS\n",
    "\n",
    "   </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "eta = 0.01\n",
    "num_classes = 10\n",
    "# the data, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# prendre moins d'images\n",
    "X_train=X_train[0:2000]\n",
    "y_train=y_train[0:2000]\n",
    "X_test=X_test[0:800]\n",
    "y_test=y_test[0:800]\n",
    "print (\"Learning on:\", X_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",X_test.shape[0],\" examples\")\n",
    "print (\"Learning on:\", X_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",X_test.shape[0],\" examples\")\n",
    "network = MyCNN()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(Relu())\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(Dropout(0.1))\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(Dense(13 * 13 * 8, num_classes,activation=\"softmax\"))\n",
    "#print(\"X_train avant datagen\",X_train)\n",
    "#cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "#opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=opt,\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape)\n",
    "img_rows, img_cols = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols,1)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print('Using real-time data augmentation.')\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)\n",
    "#print(\"X_train apres datagen\",X_train)\n",
    "\n",
    "#network.fit_generator(datagen.flow(X_train, y_train, batch_size=32),steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
    "#print(\"X_train apres datagen\",X_train)\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "#cost_history,accuracy_history=network.fit(X_train, y_train,validation_data=(X_test, y_test),epochs=epochs,workers=4)\n",
    "#cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "accuracy_test=[]\n",
    "timo = []\n",
    "t = np.zeros(len(X_test))\n",
    "\n",
    "for i in range (len(X_test)):\n",
    "    t[i] = time.time()\n",
    "\n",
    "        \n",
    "    y_pred=network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "    #timo.append(duration)\n",
    "\n",
    "\n",
    "print(\"Accuracy test: %.3f\"%np.average(accuracy_test))\n",
    "#print(\"time test: %.3f\"%np.average(timo))\n",
    "print(\"time test:\",t[len(X_test)-1]-t[0])\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
