{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Basics\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import time\n",
    "\n",
    "from keras.datasets import mnist, fashion_mnist, cifar10\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from keras.utils import to_categorical\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes :\n",
    "# Conv3x3\n",
    "class Conv3x3: # A Convolution layer using 3x3 filters.\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "\n",
    "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "        # We divide by 9 to reduce the variance of our initial values\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j                \n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        self.input = input\n",
    "        if(input.ndim == 2): # First conv we catch the image (2D)\n",
    "            self.last_input = input\n",
    "            h, w = input.shape\n",
    "            output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "            for im_region, i, j in self.iterate_regions(input):\n",
    "                output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        else: # Second conv we catch the output of maxpool (3D)\n",
    "            self.last_input = input\n",
    "            h, w , filters = input.shape\n",
    "            output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "            temp=np.transpose(input)\n",
    "            for k in range(0, filters):\n",
    "                for im_region, i, j in self.iterate_regions(temp[k]):\n",
    "                    output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "        \n",
    "        self.output = output\n",
    "        return output\n",
    "    \n",
    "    def iterate_regions_back(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        if(self.last_input.ndim == 2): # 1er conv on recup l'image 2D\n",
    "            d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "            for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "                for f in range(self.num_filters):\n",
    "                    d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "            # Update filters\n",
    "            self.filters -= learn_rate * d_L_d_filters\n",
    "            # We aren't returning anything here since we use Conv3x3 as\n",
    "            # the first layer in our CNN. Otherwise, we'd need to return\n",
    "            # the loss gradient for this layer's inputs, just like every\n",
    "            # other layer in our CNN.\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "            for im_region, i, j in self.iterate_regions_back(self.last_input):\n",
    "                h, w, f = im_region.shape\n",
    "                amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "                for i2 in range(h):\n",
    "                    for j2 in range(w):\n",
    "                        for f2 in range(f):\n",
    "                            # If this pixel was the max value, copy the gradient to it.\n",
    "                            if im_region[i2, j2, f2] == amax[f2]:\n",
    "                                d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "            return d_L_d_input\n",
    "\n",
    "# ReLU\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        output = np.maximum(0, input)  # Element-wise\n",
    "        return output\n",
    "\n",
    "    def backprop(self, dOut):\n",
    "        '''\n",
    "        fâ€²(x) = {1 if x > 0}\n",
    "                {0 otherwise}\n",
    "        '''\n",
    "        dOut[dOut <= 0] = 0\n",
    "        dOut[dOut > 0] = 1\n",
    "        \n",
    "        return dOut\n",
    "\n",
    "# MaxPool2\n",
    "class MaxPool2: # A Max Pooling layer using a pool size of 2.\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        # If this pixel was the max value, copy the gradient to it.\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input\n",
    "\n",
    "# Droupout\n",
    "class Dropout:\n",
    "    def __init__(self, drop_probability):\n",
    "        self.drop_probability = drop_probability\n",
    "    \n",
    "    def forward(self, input):\n",
    "        keep_probability = 1 - self.drop_probability\n",
    "        mask = nd.random_uniform(0, 1.0, input.shape) < keep_probability\n",
    "        mask = mask.asnumpy()\n",
    "        \n",
    "        if keep_probability > 0.0:\n",
    "            scale = (1 / keep_probability)\n",
    "        else:\n",
    "            scale = 0.0\n",
    "        out = input * mask * scale\n",
    "        return out\n",
    "\n",
    "    def backprop(self, dOut):\n",
    "        dOut[dOut <= 0] = 0\n",
    "        dOut[dOut > 0] = 1/ 1 - self.drop_probability \n",
    "        \n",
    "        return dOut\n",
    "\n",
    "# MyFlatten\n",
    "class MyFlatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        return input\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        return d_L_d_out.reshape(self.last_input_shape)\n",
    "\n",
    "# Dense\n",
    "class Dense: # A standard fully-connected layer with softmax activation.\n",
    "    def __init__(self, input_len, nodes, **kwargs):\n",
    "        self.activation_func = kwargs.get(\"activation\", None)\n",
    "        # We divide by input_len to reduce the variance of our initial values\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the softmax layer using the given input depending on its activation function.\n",
    "        Returns a 1d numpy array containing the respective probability values.\n",
    "        - input can be any array with any dimensions.\n",
    "        '''\n",
    "        self.last_input_shape = input.shape\n",
    "\n",
    "        self.last_input = input\n",
    "\n",
    "        input_len, nodes = self.weights.shape\n",
    "        \n",
    "        if(self.activation_func == 'Softmax'):\n",
    "            output = self.softmax_forward(input)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the softmax layer depending on its activation function.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float\n",
    "        '''\n",
    "        if(self.activation_func == 'Softmax'):\n",
    "            output = self.softmax_backprop(d_L_d_out, learn_rate)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def softmax_forward(self, input):\n",
    "        input_len, nodes = self.weights.shape\n",
    "\n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "\n",
    "        exp = np.exp(totals)\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "\n",
    "    def softmax_backprop(self, d_L_d_out, learn_rate):\n",
    "        # We know only 1 element of d_L_d_out will be nonzero\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^totals\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "\n",
    "            # Sum of all e^totals\n",
    "            S = np.sum(t_exp)\n",
    "\n",
    "            # Gradients of out[i] against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.last_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "\n",
    "            # Gradients of loss against totals\n",
    "            d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "            # Gradients of loss against weights/biases/input\n",
    "            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_t_d_b\n",
    "            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "            # Update weights / biases\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.biases -= learn_rate * d_L_d_b\n",
    "            return d_L_d_inputs\n",
    "\n",
    "# MyConvolutionalNeuralNetwork\n",
    "class MyConvolutionalNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.layers = []\n",
    "        \n",
    "    def info(self):\n",
    "        for i in range(len(self.layers)):\n",
    "            print(f'\\tLayer #{i + 1} => {type(self.layers[i])}')\n",
    "            if(type(self.layers[i]) is Dense):\n",
    "                print(f'\\t\\tThis layer uses the {self.layers[i].activation_func} function as its activation')\n",
    "        \n",
    "    def addLayer(self, layer):\n",
    "        self.nbLayers += 1\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        outPrevious = 0\n",
    "        for num_layer in range (0, self.nbLayers):\n",
    "            if(type(self.layers[num_layer]) is Conv3x3):\n",
    "                if num_layer == 0:\n",
    "                    X = (X / 255) - 0.5\n",
    "                    outPrevious = self.layers[num_layer].forward(X)\n",
    "                else: \n",
    "                    outPrevious = self.layers[num_layer].forward(outPrevious)\n",
    "            \n",
    "            if(type(self.layers[num_layer]) is Relu):\n",
    "                outPrevious = self.layers[num_layer].forward(outPrevious)\n",
    "                      \n",
    "            if(type(self.layers[num_layer]) is MaxPool2):\n",
    "                outPrevious = self.layers[num_layer].forward(outPrevious) \n",
    "              \n",
    "            if(type(self.layers[num_layer]) is Dropout):\n",
    "                outPrevious = self.layers[num_layer].forward(outPrevious)\n",
    "                  \n",
    "            if(type(self.layers[num_layer]) is MyFlatten):\n",
    "                outPrevious = self.layers[num_layer].forward(outPrevious) \n",
    "                \n",
    "            if(type(self.layers[num_layer]) is Dense):\n",
    "                outPrevious = self.layers[num_layer].forward(outPrevious)\n",
    "        return outPrevious\n",
    "\n",
    "    def cost_function(self, out, y):\n",
    "        return (-np.log(out[y]))\n",
    "\n",
    "    def backward_propagation(self, out, y, eta):\n",
    "        previousGradient = 0\n",
    "        for num_layer in range (self.nbLayers - 1, 0, -1):\n",
    "            if(type(self.layers[num_layer]) is Dense):\n",
    "                # Init the gradient at the end\n",
    "                gradient = np.zeros(10)\n",
    "                gradient[y] = -1 / out[y]\n",
    "                previousGradient = self.layers[num_layer].backprop(gradient, eta)\n",
    "            if(type(self.layers[num_layer]) is MyFlatten):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "            if(type(self.layers[num_layer]) is Dropout):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "            if(type(self.layers[num_layer]) is MaxPool2):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "            if(type(self.layers[num_layer]) is Relu):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "            if(type(self.layers[num_layer]) is Conv3x3):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient, eta)\n",
    "\n",
    "    def convert_prob_into_class(self, probs):\n",
    "        probs = np.copy(probs) # To not to lose props, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "    def accuracy(self, out, y):\n",
    "        acc = 1 if np.argmax(out) == y else 0\n",
    "        return acc       \n",
    "\n",
    "    def predict(self, X):\n",
    "        outPrevious = self.forward_propagation(X)\n",
    "        return outPrevious\n",
    "\n",
    "    def fit(self, X, y, *args, **kwargs):    \n",
    "        epochs = kwargs.get(\"epochs\", 20)\n",
    "        verbose = kwargs.get(\"verbose\", False)\n",
    "        eta = kwargs.get(\"eta\", 0.01)\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        t = np.zeros(epochs)\n",
    "        startTime = time.time()\n",
    "        for nb_epochs in range(epochs):\n",
    "            cost_all_images = []\n",
    "            acc_all_images = []\n",
    "            cost = 0\n",
    "            acc = 0\n",
    "            if(verbose is True):\n",
    "                print(f'\\tRunning epoch {nb_epochs + 1}...')\n",
    "            for i, (im, label) in enumerate(zip(X, y)):\n",
    "                # Do a forward pass.\n",
    "                out = self.forward_propagation(im)\n",
    "                cost += self.cost_function(out, label)\n",
    "                cost_all_images.append(self.cost_function(out, label))\n",
    "                acc += self.accuracy(out, label)\n",
    "                acc_all_images.append(self.accuracy(out, label))\n",
    "                self.backward_propagation(out, label, eta)\n",
    "                if(verbose is True):\n",
    "                    if i % 100 == 99:\n",
    "                        endTime = time.time()\n",
    "                        print(f'\\t\\t(Past 100 steps) Step {i + 1} : '\n",
    "                              f'Average Loss : {np.float64(cost / 100):.2f} '\n",
    "                              f'| Accuracy : {acc}% '\n",
    "                              f'| Time : {endTime - startTime:.2f}')\n",
    "                        cost = 0\n",
    "                        acc = 0\n",
    "                        startTime = time.time()\n",
    "            current_cost = np.average(cost_all_images)\n",
    "            cost_history.append(current_cost)  \n",
    "            current_acc = np.average(acc_all_images)\n",
    "            accuracy_history.append(current_acc)\n",
    "        return cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions :\n",
    "def plot_histories(eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification mnist data </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:44:26.935045Z",
     "start_time": "2020-05-11T13:42:00.266679Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Taking some images\n",
    "X_train = X_train[0:200]\n",
    "y_train = y_train[0:200]\n",
    "X_test = X_test[0:80]\n",
    "y_test = y_test[0:80]\n",
    "\n",
    "print(f'In this example we\\'ll '\n",
    "      f'learn on : {X_train.shape[0]} examples'\n",
    "      f' & test on : {X_test.shape[0]} examples\\n')\n",
    "\n",
    "print(f'Creating the network...')\n",
    "network = MyConvolutionalNeuralNetwork()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(Relu())\n",
    "network.addLayer(MaxPool2()) \n",
    "network.addLayer(Dropout(0.1))\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(Dense(13 * 13 * 8, 10, activation = \"Softmax\"))\n",
    "\n",
    "# Show some information of the network\n",
    "# Comment the following line to hide the network's information\n",
    "network.info()\n",
    "\n",
    "print(f'\\nLearning...')\n",
    "epochs = 2\n",
    "# Set verbose to False to hide learning's information\n",
    "cost_history, accuracy_history = network.fit(X_train, y_train, verbose = True, epochs = epochs)\n",
    "\n",
    "print(f'\\nTesting...')\n",
    "accuracy_test = []\n",
    "startTime = time.time()\n",
    "for i in range(len(X_test)):\n",
    "    y_pred = network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "endTime = time.time()\n",
    "print(f'\\tTest accuracy : {np.average(accuracy_test)}')\n",
    "print(f'\\tTest time for {X_test.shape[0]} image : {endTime - startTime:.2f}')\n",
    "\n",
    "# History display\n",
    "eta = 0.01\n",
    "plot_histories(eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification fashion mnist data </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:45:05.389271Z",
     "start_time": "2020-05-11T13:44:33.985697Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "((X_train, y_train), (X_test, y_test)) = fashion_mnist.load_data()\n",
    "\n",
    "# Taking some images\n",
    "X_train = X_train[0:200]\n",
    "y_train = y_train[0:200]\n",
    "X_test = X_test[0:80]\n",
    "y_test = y_test[0:80]\n",
    "\n",
    "print(f'In this example we\\'ll '\n",
    "      f'learn on : {X_train.shape[0]} examples'\n",
    "      f' & test on : {X_test.shape[0]} examples\\n')\n",
    "\n",
    "print(f'Creating the network...')\n",
    "network = MyConvolutionalNeuralNetwork()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(Relu())\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(Dropout(0.1))\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(Dense(13 * 13 * 8, 10, activation = \"Softmax\"))\n",
    "\n",
    "# Show some information of the network\n",
    "# Comment the following line to hide the network's information\n",
    "network.info()\n",
    "\n",
    "print(f'\\nLearning...')\n",
    "epochs = 2\n",
    "# Set verbose to False to hide learning's information\n",
    "cost_history, accuracy_history = network.fit(X_train, y_train, verbose = True, epochs = epochs)\n",
    "\n",
    "print(f'\\nTesting...')\n",
    "accuracy_test = []\n",
    "startTime = time.time()\n",
    "for i in range(len(X_test)):\n",
    "    y_pred = network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "endTime = time.time()\n",
    "print(f'\\tTest accuracy : {np.average(accuracy_test)}')\n",
    "print(f'\\tTest time for {X_test.shape[0]} image : {endTime - startTime:.2f}')\n",
    "\n",
    "# History display\n",
    "eta = 0.01\n",
    "plot_histories(eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification cifar 10 data </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The data, split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Taking some images\n",
    "X_train = X_train[0:200]\n",
    "y_train = y_train[0:200]\n",
    "X_test = X_test[0:80]\n",
    "y_test = y_test[0:80]\n",
    "\n",
    "# Editing the images\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(f'In this example we\\'ll '\n",
    "      f'learn on : {X_train.shape[0]} examples'\n",
    "      f' & test on : {X_test.shape[0]} examples\\n')\n",
    "\n",
    "print(f'Creating the network...')\n",
    "network = MyConvolutionalNeuralNetwork()\n",
    "network.addLayer(Conv3x3(16))# (32,32,16)\n",
    "network.addLayer(Relu())     # (32,32,16)\n",
    "network.addLayer(MaxPool2())  # (16,16,8)\n",
    "network.addLayer(Dropout(0.1)) # (16,16,8)\n",
    "network.addLayer(MyFlatten())  # (2048)\n",
    "network.addLayer(Dense(3600, 10, activation = \"Softmax\"))\n",
    "\n",
    "# Show some information of the network\n",
    "# Comment the following line to hide the network's information\n",
    "network.info()\n",
    "\n",
    "print(f'\\nLearning...')\n",
    "epochs = 2\n",
    "# Set verbose to False to hide learning's information\n",
    "cost_history, accuracy_history = network.fit(X_train, y_train, verbose = True, epochs = epochs)\n",
    "\n",
    "print(f'\\nTesting...')\n",
    "accuracy_test = []\n",
    "startTime = time.time()\n",
    "for i in range(len(X_test)):\n",
    "    y_pred = network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "endTime = time.time()\n",
    "print(f'\\tTest accuracy : {np.average(accuracy_test)}')\n",
    "print(f'\\tTest time for {X_test.shape[0]} image : {endTime - startTime:.2f}')\n",
    "\n",
    "# History display\n",
    "eta = 0.01\n",
    "plot_histories(eta,epochs,cost_history,accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
