{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:40:12.258597Z",
     "start_time": "2020-05-11T13:40:12.245490Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Déclarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T12:43:00.213358Z",
     "start_time": "2020-05-11T12:42:59.967458Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histories (eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3: # A Convolution layer using 3x3 filters.\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "        # We divide by 9 to reduce the variance of our initial values\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j                \n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        if(input.ndim == 2): # 1er conv on recup l'image 2D\n",
    "            self.last_input = input\n",
    "            h, w = input.shape\n",
    "            output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "            for im_region, i, j in self.iterate_regions(input):\n",
    "                output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        else: # 2ème conv on récup le resultat du maxpool 3D\n",
    "            self.last_input = input\n",
    "            h, w , filters = input.shape\n",
    "            output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "            temp=np.transpose(input)\n",
    "            for k in range(0, filters):\n",
    "                for im_region, i, j in self.iterate_regions(temp[k]):\n",
    "                    output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "                    \n",
    "        return output\n",
    "    \n",
    "    def iterate_regions_back(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        if(self.last_input.ndim == 2): # 1er conv on recup l'image 2D\n",
    "            d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "            for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "                for f in range(self.num_filters):\n",
    "                    d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "            # Update filters\n",
    "            self.filters -= learn_rate * d_L_d_filters\n",
    "            # We aren't returning anything here since we use Conv3x3 as\n",
    "            # the first layer in our CNN. Otherwise, we'd need to return\n",
    "            # the loss gradient for this layer's inputs, just like every\n",
    "            # other layer in our CNN.\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "            for im_region, i, j in self.iterate_regions_back(self.last_input):\n",
    "                h, w, f = im_region.shape\n",
    "                amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "                for i2 in range(h):\n",
    "                    for j2 in range(w):\n",
    "                        for f2 in range(f):\n",
    "                            # If this pixel was the max value, copy the gradient to it.\n",
    "                            if im_region[i2, j2, f2] == amax[f2]:\n",
    "                                d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "            return d_L_d_input\n",
    "\n",
    "class MaxPool2: # A Max Pooling layer using a pool size of 2.\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        # If this pixel was the max value, copy the gradient to it.\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input\n",
    "\n",
    "class Softmax: # A standard fully-connected layer with softmax activation.\n",
    "    def __init__(self, input_len, nodes):\n",
    "        # We divide by input_len to reduce the variance of our initial values\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the softmax layer using the given input.\n",
    "        Returns a 1d numpy array containing the respective probability values.\n",
    "        - input can be any array with any dimensions.\n",
    "        '''\n",
    "        self.last_input_shape = input.shape\n",
    "\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "\n",
    "        input_len, nodes = self.weights.shape\n",
    "\n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "\n",
    "        exp = np.exp(totals)\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "\n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the softmax layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float\n",
    "        '''\n",
    "        # We know only 1 element of d_L_d_out will be nonzero\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "\n",
    "            # e^totals\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "\n",
    "            # Sum of all e^totals\n",
    "            S = np.sum(t_exp)\n",
    "\n",
    "            # Gradients of out[i] against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.last_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "\n",
    "            # Gradients of loss against totals\n",
    "            d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "            # Gradients of loss against weights/biases/input\n",
    "            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_t_d_b\n",
    "            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "            # Update weights / biases\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.biases -= learn_rate * d_L_d_b\n",
    "            return d_L_d_inputs.reshape(self.last_input_shape)\n",
    "\n",
    "class MyFlatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        print(f'input : {input.shape}')\n",
    "        print(f'output : {input.flatten().shape}')\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        return input\n",
    "    def backprop(self, d_L_d_out):\n",
    "        return d_L_d_out.reshape(self.last_input_shape)\n",
    "\n",
    "class MyCNN:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.layers = [] # NN layers\n",
    "\n",
    "    def printLayers(self):\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(self.CNN[i].activationCNNFunc)\n",
    "        for i in range(len(self.layers)):\n",
    "            print(self.layers[i].activation_func)\n",
    "        \n",
    "    def info(self):\n",
    "        print(f'Content of the network :');\n",
    "        j = 0;\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(f'\\n\\tLayer n° {i} du CNN => ')\n",
    "            print(f'\\t\\tInput : {self.CNN[i].input}\\n\\t\\tOutput : {self.CNN[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.CNN[i].activationCNNFunc}')\n",
    "                print(f'\\t\\tW shape : {self.CNN[i].parameters[\"W\"].shape}\\n')\n",
    "                #t\\tW data :\\n{self.CNN[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.CNN[i].parameters[\"b\"].shape}\\n')\n",
    "                #\\t\\tb data :\\n{self.CNN[i].parameters[\"b\"]}')\n",
    "                \n",
    "        for i in range(len(self.layers)):\n",
    "            print(f'\\n\\tLayer n° {i} du NN => ')\n",
    "            print(f'\\t\\tInput : {self.layers[i].input}\\n\\t\\tOutput : {self.layers[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "                print(f'\\t\\tW shape : {self.layers[i].parameters[\"W\"].shape}\\n')\n",
    "                #\\t\\tW data :\\n{self.layers[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.layers[i].parameters[\"b\"].shape}\\n')\n",
    "                #\\t\\tb data :\\n{self.layers[i].parameters[\"b\"]}')\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        self.nbLayers += 1 # Le layer 0 = input, si CNN = images\n",
    "        #print (\"layer = \",type(self.layers))\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "     \n",
    "    def forward_propagation(self, X):\n",
    "        outPrevious=0\n",
    "        for num_layer in range (0, self.nbLayers):\n",
    "            #print (\"\\nPredict: lecture objet\",self.layers[num_layer])\n",
    "            if (type(self.layers[num_layer]) is Conv3x3):\n",
    "                if num_layer==0:\n",
    "                    X=(X / 255) - 0.5\n",
    "                    outPrevious=self.layers[num_layer].forward(X)\n",
    "                else : \n",
    "                    outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "                #print (\"\\tpasse dans Conv3x3 - taille en sortie\",outPrevious.shape)\n",
    "            if (type(self.layers[num_layer]) is MaxPool2):\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "                #print (\"\\tpasse dans MaxPool2 - taille en sortie\",outPrevious.shape)\n",
    "            if (type(self.layers[num_layer]) is MyFlatten):\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious) \n",
    "                #print (\"\\tpasse dans Flatten - taille en sortie\",outPrevious.shape)\n",
    "            if (type(self.layers[num_layer]) is Softmax):\n",
    "                #print (\"\\tpasse dans Softmax - taille en sortie\",outPrevious.shape)\n",
    "                outPrevious=self.layers[num_layer].forward(outPrevious)\n",
    "        return outPrevious\n",
    "               \n",
    "        \n",
    "        \n",
    "    def cost_function(self, out,y):\n",
    "        #print(\"cost_function\", y.shape,out.shape)\n",
    "        ##sortie=self.layers[self.nbLayers-1].parameters['A'].flatten()\n",
    "        #print (\"sortie.shape\",sortie.shape)\n",
    "        #return (-(y * np.log(out + 1e-8) + (1 - y) * np.log(1 - out + 1e-8))).mean()\n",
    "        #return (-(y * np.log(out + 1e-8) + (1 - y) * np.log(1 - out + 1e-8))).mean()\n",
    "        return (-np.log(out[y]))\n",
    "\n",
    "    def backward_propagation(self, out,y,eta):\n",
    "        previousGradient=0\n",
    "        for num_layer in range (self.nbLayers-1,0,-1):\n",
    "            if (type(self.layers[num_layer]) is Softmax):\n",
    "                # initialisation du gradient à la fin\n",
    "                gradient = np.zeros(10)\n",
    "                gradient[y] = -1 / out[y]\n",
    "                previousGradient = self.layers[num_layer].backprop(gradient, eta)\n",
    "                #print (\"backpropagation - softmax\", previousGradient.shape)\n",
    "            if (type(self.layers[num_layer]) is MaxPool2):\n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient)\n",
    "                #print (\"backpropagation - pool\", previousGradient.shape) \n",
    "            if (type(self.layers[num_layer]) is Conv3x3):\n",
    "                #print (\"backpropagation - Conv 3x3\")        \n",
    "                previousGradient = self.layers[num_layer].backprop(previousGradient, eta)\n",
    " \n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "\n",
    "\n",
    "    def accuracy(self,out, y):\n",
    "        acc = 1 if np.argmax(out) == y else 0\n",
    "        return acc       \n",
    "\n",
    "    def predict(self, X):\n",
    "        outPrevious=self.forward_propagation(X)\n",
    "        return outPrevious\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        for nb_epochs in range(epochs):\n",
    "            cost_all_images=[]\n",
    "            acc_all_images=[]\n",
    "            cost = 0\n",
    "            acc = 0\n",
    "            print (\"run epoch: \",nb_epochs+1)\n",
    "            for i, (im, label) in enumerate(zip(X, y)):\n",
    "                # Do a forward pass.\n",
    "                out=self.forward_propagation(im)\n",
    "                cost+=self.cost_function(out,label)\n",
    "                cost_all_images.append(self.cost_function(out,label))\n",
    "                acc+=self.accuracy(out, label)\n",
    "                acc_all_images.append(self.accuracy(out, label))\n",
    "                self.backward_propagation(out,label,eta)\n",
    "                if verbose:\n",
    "                    if i % 100 == 99:\n",
    "                        print(\n",
    "                        '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "                        (i + 1, cost / 100, acc)\n",
    "                        )\n",
    "                        cost = 0\n",
    "                        acc = 0\n",
    "            current_cost=np.average(cost_all_images)\n",
    "            cost_history.append(current_cost)  \n",
    "            current_acc=np.average(acc_all_images)\n",
    "            accuracy_history.append(current_acc)\n",
    "        \n",
    "        return cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de mnist </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:44:26.935045Z",
     "start_time": "2020-05-11T13:42:00.266679Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = mnist.test_images()[:800]\n",
    "test_labels = mnist.test_labels()[:800]\n",
    "\n",
    "# Jeu d'apprentissage 60%\n",
    "validation_size = 0.65\n",
    "\n",
    "# 40% du jeu de données pour le test\n",
    "testsize = 1 - validation_size\n",
    "\n",
    "seed = 30\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_images, test_labels, train_size = validation_size, random_state = seed, test_size = testsize)\n",
    "\n",
    "print (\"Learning on:\", X_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",X_test.shape[0],\" examples\")\n",
    "\n",
    "epochs = 12\n",
    "eta = 0.01\n",
    "num_classes = 10\n",
    "\n",
    "network = MyCNN()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(Softmax(13 * 13 * 8, num_classes))\n",
    "\n",
    "cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "accuracy_test=[]\n",
    "\n",
    "for i in range (len(X_test)):\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "    \n",
    "print(\"Accuracy test: %.3f\"%np.average(accuracy_test))\n",
    "\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de fashion mnist </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:45:05.389271Z",
     "start_time": "2020-05-11T13:44:33.985697Z"
    }
   },
   "outputs": [],
   "source": [
    "((X_train, y_train), (X_test, y_test)) = fashion_mnist.load_data()\n",
    "\n",
    "# prendre moins d'images\n",
    "X_train=X_train[0:200]\n",
    "y_train=y_train[0:200]\n",
    "X_test=X_test[0:10]\n",
    "y_test=y_test[0:10]\n",
    "\n",
    "print (\"Learning on:\", X_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",X_test.shape[0],\" examples\")\n",
    "\n",
    "epochs = 8\n",
    "eta = 0.01\n",
    "network = MyCNN()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(Softmax(13 * 13 * 8, 10))\n",
    "cost_history,accuracy_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "accuracy_test=[]\n",
    "for i in range (len(X_test)):\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    acc_test = network.accuracy(y_pred, y_test[i])\n",
    "    accuracy_test.append(acc_test)\n",
    "    \n",
    "print(\"Accuracy test: %.3f\"%np.average(accuracy_test))\n",
    "\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Keras\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T13:46:36.369834Z",
     "start_time": "2020-05-11T13:46:32.338358Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train=x_train[0:520]\n",
    "y_train=y_train[0:520]\n",
    "x_test=x_test[0:280]\n",
    "y_test=y_test[0:280]\n",
    "\n",
    "print (\"Learning on:\", x_train.shape[0], \" examples\")\n",
    "print (\"Test on:\",x_test.shape[0],\" examples\")\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
