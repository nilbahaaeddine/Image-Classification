{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T11:51:17.978410Z",
     "start_time": "2020-05-09T11:51:13.845326Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Déclarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T11:51:18.185828Z",
     "start_time": "2020-05-09T11:51:17.981845Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histories (eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})\n",
    "\n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis = 0) - 0.1\n",
    "    amax, bmax = X.max(axis = 0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "\n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap = cm, alpha = 0.8)\n",
    "\n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y, cmap = cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01, x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x <= 0] = 0.01\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx / expx.sum(axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T10:59:21.198003Z",
     "start_time": "2020-05-10T10:59:13.558370Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv3x3: # A Convolution layer using 3x3 filters.\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        print (\"Creation d'un Conv3x3 avec \", num_filters)\n",
    "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "        # We divide by 9 to reduce the variance of our initial values\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        #on doit faire ne sorte que l'entrée soit un tableau 3D non 2d\n",
    "        print (\"Je suis dans le forward de Conv3x3\")\n",
    "        self.last_input = input\n",
    "        print(input.shape)\n",
    "        #h, w = input.shape\n",
    "        h=input.shape[0]\n",
    "        w=input.shape[1]\n",
    "        output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "          for f in range(self.num_filters):\n",
    "            d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "        # Update filters\n",
    "        self.filters -= learn_rate * d_L_d_filters\n",
    "\n",
    "        # We aren't returning anything here since we use Conv3x3 as the first layer in our CNN.\n",
    "        # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
    "        # other layer in our CNN.\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "class MaxPool2:\n",
    "    # A Max Pooling layer using a pool size of 2.\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        print (\"je suis dans le forward de MaxPool2\")\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "          h, w, f = im_region.shape\n",
    "          amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "          for i2 in range(h):\n",
    "            for j2 in range(w):\n",
    "              for f2 in range(f):\n",
    "                # If this pixel was the max value, copy the gradient to it.\n",
    "                if im_region[i2, j2, f2] == amax[f2]:\n",
    "                  d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input\n",
    "    \n",
    "class MyFlatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        print(f'input : {input.shape}')\n",
    "        print(f'output : {input.flatten().shape}')\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        return input\n",
    "    def backprop(self, d_L_d_out):\n",
    "        return d_L_d_out.reshape(self.last_input_shape)\n",
    "    \n",
    "class MyLayer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.input = kwargs.get(\"input\", None) # Number of neurons at layer i-1\n",
    "        self.output = kwargs.get(\"output\", None) # Number of neurons at  layer i (current layer) \n",
    "        self.activ_function_curr = kwargs.get(\"activation\", None) # Activation function for the layer\n",
    "        self.paramCouche = kwargs.get(\"paramCouche\", None) # Param de la couche Conv3x3\n",
    "        self.type = kwargs.get(\"type\", None)\n",
    "        self.couche = kwargs.get(\"couche\", None)\n",
    "        self.parameters = {}\n",
    "        self.derivatives = {}\n",
    "        self.activation_func = None        \n",
    "        \n",
    "        if self.activ_function_curr == \"relu\":\n",
    "                self.activation_func = relu\n",
    "                self.backward_activation_func = relu_prime\n",
    "        elif self.activ_function_curr == \"sigmoid\":\n",
    "                self.activation_func = sigmoid\n",
    "                self.backward_activation_func = sigmoid_prime\n",
    "        elif self.activ_function_curr == \"tanh\":\n",
    "                self.activation_func = tanh\n",
    "                self.backward_activation_func = tanh_prime\n",
    "        elif self.activ_function_curr == \"leakyrelu\":\n",
    "                self.activation_func = leakyrelu\n",
    "                self.backward_activation_func = leakyrelu_prime\n",
    "        elif self.activ_function_curr == \"softmax\":\n",
    "                self.activation_func = softmax\n",
    "                self.backward_activation_func = softmax\n",
    "\n",
    "    def initParams(self):\n",
    "        # Initialisation du dictionnaire de données parameters contenant W, A et Z pour un layer\n",
    "        print (\"Je suis dans init Params, là je mets des W et b avec des randoms\")\n",
    "        seed = 30\n",
    "        np.random.seed(seed)\n",
    "        self.parameters['W'] = np.random.randn(self.output, self.input) * np.sqrt(2 / self.input)\n",
    "        self.parameters['b'] = np.random.randn(self.output, 1) * 0.1\n",
    "\n",
    "    def setW(self, matW):\n",
    "        self.parameters['W'] = np.copy(matW)\n",
    "\n",
    "    def setA(self, matA):\n",
    "        self.parameters['A'] = np.copy(matA) \n",
    "\n",
    "    def setZ(self, matZ):\n",
    "        self.parameters['Z'] = np.copy(matZ)\n",
    "\n",
    "    def setB(self, matB):\n",
    "        self.parameters['b'] = np.copy(matB)\n",
    "\n",
    "    def setdW(self, matdW):\n",
    "        self.parameters['dW'] = np.copy(matdW)\n",
    "\n",
    "    def setdA(self, matdA):\n",
    "        self.parameters['dA'] = np.copy(matdA)\n",
    "\n",
    "    def setdZ(self, matdZ):\n",
    "        self.parameters['dZ'] = np.copy(matdZ)\n",
    "\n",
    "    def setdB(self, matdB):\n",
    "        self.parameters['db'] = np.copy(matdB)\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.nbCNNlayers = 0\n",
    "        self.layers = [] # NN layers\n",
    "        #self.CNN = [] # CNN layers\n",
    "        self.CNN_EXIST=False\n",
    "\n",
    "    def info(self):\n",
    "        print(f'Content of the network :')\n",
    "        for i in range(len(self.layers)):\n",
    "            if self.CNN_EXIST != True:\n",
    "                for i in range(len(self.layers)):\n",
    "                    print(f'\\n\\tLayer n° {i} du NN => ')\n",
    "                    print(f'\\t\\tInput : {self.layers[i].input}\\n\\t\\tOutput : {self.layers[i].output}')\n",
    "                    if (i != 0):\n",
    "                        print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "                        print(f'\\t\\tW shape : {self.layers[i].parameters[\"W\"].shape}\\n\\t\\tW data :\\n{self.layers[i].parameters[\"W\"]}')\n",
    "                        print(f'\\t\\tb shape : {self.layers[i].parameters[\"b\"].shape}\\n\\t\\tb data :\\n{self.layers[i].parameters[\"b\"]}')\n",
    "            else:\n",
    "                if(type(self.layers[i]) is not MyLayer):\n",
    "                    print(f'\\n\\tLayer n° {i} (CNN) => ')\n",
    "                    print(f'\\t\\tCouche : {self.layers[i]}')\n",
    "                else:\n",
    "                    print(f'\\n\\tLayer n° {i} (NN) => ')\n",
    "                    print(f'\\t\\tOutput : {self.layers[i].output}')\n",
    "                    print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        self.nbLayers += 1 # Le layer 0 = input, si CNN = images\n",
    "        print (\"Layer\",layer)\n",
    "        if (type(layer) is Conv3x3):\n",
    "            print (\"c'est un 3x3\")\n",
    "            self.layers.append(layer)\n",
    "            self.CNN_EXIST=True # il existe un CNN donc pour le RNN il faudra prendre la sortie du flatten\n",
    "        if (type(layer) is MaxPool2):    \n",
    "            print (\"c'est un maxPool2\")\n",
    "            self.layers.append(layer)\n",
    "        if (type(layer) is MyFlatten):    \n",
    "            print (\"c'est un Flatten\")\n",
    "            self.layers.append(layer)\n",
    "        if (type(layer) is MyLayer):    \n",
    "            print (\"c'est un MyLayer\") \n",
    "            self.layers.append(layer)           \n",
    "            # attention s'il y a un CNN on n'a pas initialisé les valeurs\n",
    "            # il faut le faire dans le predict avec un init_Params()\n",
    "            if self.CNN_EXIST != True: # on est dans le cas normal où il n'y a pas de CNN\n",
    "                if (self.nbLayers==1): \n",
    "                    # this is the first layer so adding a layer 0\n",
    "                    layerZero=Layer(layer.input)\n",
    "                    self.layers.append(layerZero)\n",
    "            \n",
    "                self.layers.append(layer) \n",
    "                self.layers[self.nbLayers].input=self.layers[self.nbLayers-1].output\n",
    "                layer.initParams()\n",
    "           #si on a un CNN, il faut récupérer la valeur du flatten pour le forward\n",
    "                    \n",
    "          \n",
    "\n",
    "    def set_parametersW_b (self, numlayer, matX, matb):\n",
    "        self.layers[numlayer].parameters['W'] = np.copy(matX)\n",
    "        self.layers[numlayer].parameters['b'] = np.copy(matb)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        outConv=0\n",
    "        outMaxPool=0\n",
    "        outFlatten=0\n",
    "        previousFlatten=False\n",
    "        for num_layer in range (0, self.nbLayers):\n",
    "            print (\"\\nPredict: lecture objet\",self.layers[num_layer])\n",
    "            if (type(self.layers[num_layer]) is Conv3x3):\n",
    "                outConv=self.layers[num_layer].forward(X)\n",
    "                print (\"\\tpasse dans Conv3x3\")\n",
    "            if (type(self.layers[num_layer]) is MaxPool2):\n",
    "                outMaxPool=self.layers[num_layer].forward(outConv)\n",
    "                print (\"\\tpasse dans MaxPool2\")\n",
    "            if (type(self.layers[num_layer]) is MyFlatten):\n",
    "                outFlatten=self.layers[num_layer].forward(outMaxPool) \n",
    "                print (\"\\tpasse dans Flatten\")\n",
    "                previousFlatten=True\n",
    "            if (type(self.layers[num_layer]) is MyLayer):\n",
    "                # je viens de passer dans le flatten,récupération de l'input\n",
    "                if (previousFlatten): # le précédent est flatten\n",
    "                    print (\"len Flatten\", len(outFlatten))\n",
    "                    #self.layers[num_layer].input = self.layers[self.nbLayers - 1].output\n",
    "                    #layer.initParams()\n",
    "                    #il faut initialiser les valeurs des paramètres\n",
    "                    print (\"Init des paramètres. Output = \",self.layers[num_layer].output)\n",
    "                    print (\"Le input doit prendre le flatten\")\n",
    "                    self.layers[num_layer].input=len(outFlatten)\n",
    "                    print (\"On relance init Param pour créer les W et b\")\n",
    "                    self.layers[num_layer].initParams()\n",
    "                    self.layers[num_layer].setZ(np.dot(self.layers[num_layer].parameters['W'], \n",
    "                                       outFlatten) + self.layers[num_layer].parameters['b'])\n",
    "                    self.layers[num_layer].setA(self.layers[num_layer].activation_func(self.layers[num_layer].parameters['Z']))\n",
    "                    print(\"self.nbLayers - activation\",self.layers[num_layer].activation_func)\n",
    "                    previousFlatten=False\n",
    "                else: # là le précédent n'est plus flatten je suis dans le RNN\n",
    "                    print (\"ICI je suis dans un RNN qui n'est pas après un flatten\")\n",
    "                    # il faut initialiser les valeurs de W et b\n",
    "                    print (\"Init des paramètres. Output = \",self.layers[num_layer].output)\n",
    "                    print (\"Le input doit prendre le layer precedent\",self.layers[num_layer-1].output)\n",
    "                    self.layers[num_layer].input=self.layers[num_layer-1].output\n",
    "                    self.layers[num_layer].initParams()\n",
    "                    self.layers[num_layer].setZ(np.dot(self.layers[num_layer].parameters['W'], \n",
    "                                       self.layers[num_layer - 1].parameters['A']) + self.layers[num_layer].parameters['b'])\n",
    "                    # Applying the activation function of the layer to Z\n",
    "                    self.layers[num_layer].setA(self.layers[num_layer].activation_func(self.layers[num_layer].parameters['Z']))\n",
    "                    #self.activation_func = softmax\n",
    "                    print(\"self.nbLayers - activation\",self.layers[num_layer].activation_func)\n",
    "                    print (\"Taille A \",self.layers[num_layer].parameters['A'].shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def cost_function(self, y):\n",
    "        print(\"cost_function\")\n",
    "        return (-(y * np.log(self.layers[self.nbLayers-1].parameters['A'] + 1e-8) + (1 - y) * np.log(1 - self.layers[self.nbLayers-1].parameters['A'] + 1e-8))).mean()\n",
    "\n",
    "    def backward_propagation(self, y):\n",
    "        #calcul de dZ dW et db pour le dernier layer\n",
    "        print(\"self.layers[self.nbLayers].parameters['A']\",self.layers[self.nbLayers].parameters['A'])\n",
    "        print(\"y\",y)\n",
    "        self.layers[self.nbLayers].derivatives['dZ']=self.layers[self.nbLayers].parameters['A']-y\n",
    "        self.layers[self.nbLayers].derivatives['dW']=np.dot(self.layers[self.nbLayers].derivatives['dZ'],\n",
    "                                                             np.transpose(self.layers[self.nbLayers-1].parameters['A']))\n",
    "        m=self.layers[self.nbLayers].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "        self.layers[self.nbLayers].derivatives['db']=np.sum(self.layers[self.nbLayers].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m\n",
    "        \n",
    "        #calcul de dZ dW db pour les autres layers\n",
    "        for l in range(self.nbLayers-1,0,-1) :\n",
    "            self.layers[l].derivatives['dZ']=np.dot(np.transpose(self.layers[l+1].parameters['W']),\n",
    "                                            self.layers[l+1].derivatives['dZ'])*self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "            \n",
    "            self.layers[l].derivatives[\"dW\"]=np.dot(self.layers[l].derivatives['dZ'],\n",
    "                                            np.transpose(self.layers[l-1].parameters['A']))\n",
    "                       \n",
    "            m=self.layers[l-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "            self.layers[l].derivatives['db']=np.sum(self.layers[l].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m    \n",
    "                      \n",
    "        outputBackprop = self.CNN[self.nbCNNlayers].backward_activation_func(self.layers[0].derivatives)\n",
    "        for l in range(self.nbCNNlayers-1,1,-1) :\n",
    "            outputBackprop = self.CNN[l].backward_activation_func(outputBackprop)\n",
    "                      \n",
    "    def update_parameters(self, eta) :\n",
    "        for l in range(1,self.nbLayers+1) :\n",
    "            self.layers[l].parameters['W']-=eta*self.layers[l].derivatives['dW']\n",
    "            self.layers[l].parameters[\"b\"]-=eta*self.layers[l].derivatives[\"db\"]\n",
    "\n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "    def plot_W_b_epoch (self,epoch,parameter_history):\n",
    "        mat=[]\n",
    "        max_size_layer=0\n",
    "        for l in range(1, self.nbLayers+1):    \n",
    "            value=parameter_history[epoch]['W'+str(l)]\n",
    "            if (parameter_history[epoch]['W'+str(l)].shape[1]>max_size_layer):\n",
    "                max_size_layer=parameter_history[epoch]['W'+str(l)].shape[1]\n",
    "            mat.append(value)\n",
    "        figure=plt.figure(figsize=((self.nbLayers+1)*3,int (max_size_layer/2)))    \n",
    "        for nb_w in range (len(mat)):    \n",
    "                plt.subplot(1, len(mat), nb_w+1)\n",
    "                plt.matshow(mat[nb_w],cmap = plt.cm.gist_rainbow,fignum=False, aspect='auto')\n",
    "                plt.colorbar()    \n",
    "        thelegend=\"Epoch \"+str(epoch)\n",
    "        plt.title (thelegend)    \n",
    "\n",
    "    def accuracy(self,y_hat, y):\n",
    "        if self.layers[self.nbLayers-1].activation_func==softmax:\n",
    "            # si la fonction est softmax, les valeurs sont sur différentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # où est la valeur maximale à la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded=np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis=0)\n",
    "            y_encoded=np.copy(y)\n",
    "            y_encoded=np.argmax(y_encoded, axis=0)\n",
    "            return (y_hat_encoded == y_encoded).mean()\n",
    "        # la dernière fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilité du résultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()       \n",
    "\n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.layers[self.nbLayers-1].parameters['A']\n",
    "\n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        print (\"m ds next_batch\",m)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        batchsize=kwargs.get(\"batchsize\",32)\n",
    "        print (\"Dans fit, X.shape, y.shape\",X.shape,len(X),y.shape,y[0:2])\n",
    "    #def fit(self, X, y, epochs, eta = 0.01,batchsize=64) :\n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        parameter_history = []\n",
    "        for i in range(epochs):\n",
    "            i+=1\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost = []\n",
    "            accuracy = []\n",
    "            ##########\n",
    "            # on ne va passer qu'une image à la fois\n",
    "            for nb_x in range(y.shape[0]):\n",
    "                self.forward_propagation(X[nb_x])\n",
    "                #out=self.layers[self.nbLayers-1].parameters['A']\n",
    "                #loss = -np.log(out[y[nb_x]])\n",
    "                #acc = 1 if np.argmax(out) == y[nb_x] else 0\n",
    "                print (nb_x,\" calcul du cout pour \")\n",
    "                print (y[nb_x])\n",
    "                current_cost=self.cost_function(y[nb_x])\n",
    "                print (\"\\t\\tcurrent_cost\",current_cost)\n",
    "                \n",
    "                cost.append(current_cost)\n",
    "                y_hat = self.predict(X[nb_x])\n",
    "                current_accuracy = self.accuracy(y_hat,y[nb_x] )\n",
    "                print (\"\\t\\tPour y[\",nb_x,\"]\",y[nb_x],\" valeur prédite\",y_hat)\n",
    "                \n",
    "                ############## TODO ###########\n",
    "                # Revoir si on récupère bien l'accuracy \n",
    "                # Faire la backpropagation + update parameters\n",
    "                    # Extraction et traitement d'un batch à la fois\n",
    "                '''\n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                if self.layers[self.nbLayers-1].activation_func==softmax:\n",
    "                    # la classification n'est pas binaire, y a utilisé one-hot-encoder\n",
    "                    # le batchy doit donc être transposé et le résultat doit\n",
    "                    # être sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                    print(\"self.layers[self.nbLayers].activation_func==softmax\")\n",
    "                    print(\"batchy.shape[0]\",batchy.shape[0],batchy.shape[1])\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe pas\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                    print(\"else self.layers[self.nbLayers].activation_func==softmax\")\n",
    "                #batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                print (\"batchy\",batchy)\n",
    "                self.forward_propagation(batchX)\n",
    "                #self.backward_propagation(batchy)\n",
    "                #self.update_parameters(eta)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                current_cost=self.cost_function(batchy)\n",
    "                print (\"current_cost\",current_cost)\n",
    "                cost_batch.append(current_cost)\n",
    "                y_hat = self.predict(batchX)\n",
    "                current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                accuracy_batch.append(current_accuracy)\n",
    "                '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            ##############\n",
    "            \n",
    "                \n",
    "            #parameter_history.append(save_values)        \n",
    "            # sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            #current_cost=np.average(cost_batch)\n",
    "            #cost_history.append(current_cost)\n",
    "            #current_accuracy=np.average(accuracy_batch)\n",
    "            #accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(verbose == True):\n",
    "                print(f'Epoch : {i}/{epochs} | cost : {float(current_cost)}| accuracy : {current_accuracy}')\n",
    "              \n",
    "        return self.layers, cost_history, accuracy_history, parameter_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification des données de mnist </h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atemp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-10T10:59:21.893627Z",
     "start_time": "2020-05-10T10:59:21.201060Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = mnist.test_images()[:10]\n",
    "test_labels = mnist.test_labels()[:10]\n",
    "\n",
    "\n",
    "# mis au bon format des images\n",
    "newTestImages = []\n",
    "for image in test_images:\n",
    "    newTestImages.append(((image / 255) - 0.5))\n",
    "\n",
    "X = np.float64(newTestImages)\n",
    "y = to_categorical(test_labels, num_classes=10)   \n",
    "print (\"transformation des y pour etre au bon format\", y.shape)\n",
    "print (\"exemple des donnees de y\",y[0:3])\n",
    "\n",
    "# Jeu d'apprentissage 60%\n",
    "validation_size = 0.6\n",
    "\n",
    "# 40% du jeu de données pour le test\n",
    "testsize = 1 - validation_size\n",
    "\n",
    "seed = 30\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = validation_size, random_state = seed, test_size = testsize)\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prédictives)\n",
    "X_train=np.transpose(X_train)\n",
    "print (\"X_train.shape, y_train.shape\", X_train.shape,y_train.shape)\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "\n",
    "# ATTENTION DANS LE MODELE COMME ON PASSE UNE IMAGE A LA FOIS IL FAUT QUE LE OUTPUT soit à 1\n",
    "# cf ci-dessous\n",
    "network = MyNeuralNetwork()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(MyLayer(output = 1, activation = \"relu\"))\n",
    "network.addLayer(MyLayer(output = 10, activation = \"softmax\"))\n",
    "\n",
    "network.info()\n",
    "\n",
    "epochs = 5\n",
    "eta = 0.01\n",
    "batchsize=20\n",
    "\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "print(f'shape de X_train')\n",
    "print(X_train.shape)\n",
    "print(f'shape de X_test')\n",
    "print(X_test.shape)\n",
    "print(f'shape de y_train')\n",
    "print(y_train.shape)\n",
    "print (y_train)\n",
    "print(f'shape de y_test')\n",
    "print(y_test.shape)\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "\n",
    "#Entraînement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "##### Attention les fonctions en dessous ne marcheront pas car je n'ai pas sauvegardé l'historique\n",
    "# pour la prédiction il faut s'inspirer de y_hat = self.predict(X[nb_x])\n",
    "# pour l'instant predict attend un tableau, là il faut passer valeur par valeur\n",
    "#Prédiction\n",
    "for i in range (y_test.shape[0]):\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    accuracy_test = network.accuracy(y_pred, y_test[i])\n",
    "    print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "#y_pred=network.predict(X_test)\n",
    "#accuracy_test = network.accuracy(y_pred, y_test)\n",
    "#print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "\n",
    "# Affichage des historiques\n",
    "#plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
e forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 0 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "1  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 1 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "2  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 2 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "3  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 3 ] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "4  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 4 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "5  calcul du cout pour \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3288062434554175\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 5 ] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]  valeur prédite [[0.08622587]\n",
      " [0.13952436]\n",
      " [0.07287609]\n",
      " [0.10068638]\n",
      " [0.10987578]\n",
      " [0.11307331]\n",
      " [0.06449763]\n",
      " [0.14755441]\n",
      " [0.081897  ]\n",
      " [0.08378917]]\n",
      "Epoch : 1/5 | cost : 0.3288062434554175| accuracy : 0.0\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "0  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 0 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "1  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 1 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "2  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 2 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "3  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 3 ] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "4  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 4 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "5  calcul du cout pour \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3288062434554175\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 5 ] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]  valeur prédite [[0.08622587]\n",
      " [0.13952436]\n",
      " [0.07287609]\n",
      " [0.10068638]\n",
      " [0.10987578]\n",
      " [0.11307331]\n",
      " [0.06449763]\n",
      " [0.14755441]\n",
      " [0.081897  ]\n",
      " [0.08378917]]\n",
      "Epoch : 2/5 | cost : 0.3288062434554175| accuracy : 0.0\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "0  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 0 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "1  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 1 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "2  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 2 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "3  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 3 ] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "4  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 4 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "5  calcul du cout pour \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3288062434554175\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 5 ] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]  valeur prédite [[0.08622587]\n",
      " [0.13952436]\n",
      " [0.07287609]\n",
      " [0.10068638]\n",
      " [0.10987578]\n",
      " [0.11307331]\n",
      " [0.06449763]\n",
      " [0.14755441]\n",
      " [0.081897  ]\n",
      " [0.08378917]]\n",
      "Epoch : 3/5 | cost : 0.3288062434554175| accuracy : 0.0\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "0  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 0 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "1  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 1 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "2  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 2 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "3  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 3 ] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "4  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 4 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "5  calcul du cout pour \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3288062434554175\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 5 ] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]  valeur prédite [[0.08622587]\n",
      " [0.13952436]\n",
      " [0.07287609]\n",
      " [0.10068638]\n",
      " [0.10987578]\n",
      " [0.11307331]\n",
      " [0.06449763]\n",
      " [0.14755441]\n",
      " [0.081897  ]\n",
      " [0.08378917]]\n",
      "Epoch : 4/5 | cost : 0.3288062434554175| accuracy : 0.0\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "0  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 0 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "1  calcul du cout pour \n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 1 ] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "2  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 2 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "3  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3263132368434246\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 3 ] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "4  calcul du cout pour \n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.32631323684342456\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 4 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  valeur prédite [[0.1010828 ]\n",
      " [0.11735313]\n",
      " [0.08250394]\n",
      " [0.09603369]\n",
      " [0.11216525]\n",
      " [0.11000842]\n",
      " [0.07988026]\n",
      " [0.12326596]\n",
      " [0.08129947]\n",
      " [0.09640707]]\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "5  calcul du cout pour \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cost_function\n",
      "\t\tcurrent_cost 0.3288062434554175\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 6)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 2, 8)\n",
      "output : (208,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 208\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "\t\tPour y[ 5 ] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]  valeur prédite [[0.08622587]\n",
      " [0.13952436]\n",
      " [0.07287609]\n",
      " [0.10068638]\n",
      " [0.10987578]\n",
      " [0.11307331]\n",
      " [0.06449763]\n",
      " [0.14755441]\n",
      " [0.081897  ]\n",
      " [0.08378917]]\n",
      "Epoch : 5/5 | cost : 0.3288062434554175| accuracy : 0.0\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 4)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 1, 8)\n",
      "output : (104,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 104\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "Accuracy test: 1.000\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 4)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 1, 8)\n",
      "output : (104,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 104\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "Accuracy test: 0.000\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 4)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 1, 8)\n",
      "output : (104,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 104\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "Accuracy test: 0.000\n",
      "\n",
      "Predict: lecture objet <__main__.Conv3x3 object at 0x7f4b4173acd0>\n",
      "Je suis dans le forward de Conv3x3\n",
      "(28, 4)\n",
      "\tpasse dans Conv3x3\n",
      "\n",
      "Predict: lecture objet <__main__.MaxPool2 object at 0x7f4bd8ad7550>\n",
      "je suis dans le forward de MaxPool2\n",
      "\tpasse dans MaxPool2\n",
      "\n",
      "Predict: lecture objet <__main__.MyFlatten object at 0x7f4b4173ae50>\n",
      "input : (13, 1, 8)\n",
      "output : (104,)\n",
      "\tpasse dans Flatten\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4bd8b3df10>\n",
      "len Flatten 104\n",
      "Init des paramètres. Output =  1\n",
      "Le input doit prendre le flatten\n",
      "On relance init Param pour créer les W et b\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function relu at 0x7f4b417f6680>\n",
      "\n",
      "Predict: lecture objet <__main__.MyLayer object at 0x7f4b4172b0d0>\n",
      "ICI je suis dans un RNN qui n'est pas après un flatten\n",
      "Init des paramètres. Output =  10\n",
      "Le input doit prendre le layer precedent 1\n",
      "Je suis dans init Params, là je mets des W et b avec des randoms\n",
      "self.nbLayers - activation <function softmax at 0x7f4b417f6b00>\n",
      "Taille A  (10, 1)\n",
      "Accuracy test: 0.000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a6e104d3bd04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Affichage des historiques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mplot_histories\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8b66db13b1fb>\u001b[0m in \u001b[0;36mplot_histories\u001b[0;34m(eta, epochs, cost_history, accuracy_history)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"$\\eta$ :{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mline1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cost'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mline2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mline1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHandlerLine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_images = mnist.test_images()[:10]\n",
    "test_labels = mnist.test_labels()[:10]\n",
    "\n",
    "\n",
    "# mis au bon format des images\n",
    "newTestImages = []\n",
    "for image in test_images:\n",
    "    newTestImages.append(((image / 255) - 0.5))\n",
    "\n",
    "X = np.float64(newTestImages)\n",
    "y = to_categorical(test_labels, num_classes=10)   \n",
    "print (\"transformation des y pour etre au bon format\", y.shape)\n",
    "print (\"exemple des donnees de y\",y[0:3])\n",
    "\n",
    "# Jeu d'apprentissage 60%\n",
    "validation_size = 0.6\n",
    "\n",
    "# 40% du jeu de données pour le test\n",
    "testsize = 1 - validation_size\n",
    "\n",
    "seed = 30\n",
    "\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = validation_size, random_state = seed, test_size = testsize)\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prédictives)\n",
    "X_train=np.transpose(X_train)\n",
    "print (\"X_train.shape, y_train.shape\", X_train.shape,y_train.shape)\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "\n",
    "# ATTENTION DANS LE MODELE COMME ON PASSE UNE IMAGE A LA FOIS IL FAUT QUE LE OUTPUT soit à 1\n",
    "# cf ci-dessous\n",
    "network = MyNeuralNetwork()\n",
    "network.addLayer(Conv3x3(8))\n",
    "network.addLayer(MaxPool2())\n",
    "network.addLayer(MyFlatten())\n",
    "network.addLayer(MyLayer(output = 1, activation = \"relu\"))\n",
    "network.addLayer(MyLayer(output = 10, activation = \"softmax\"))\n",
    "\n",
    "network.info()\n",
    "\n",
    "epochs = 5\n",
    "eta = 0.01\n",
    "batchsize=20\n",
    "\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "print(f'shape de X_train')\n",
    "print(X_train.shape)\n",
    "print(f'shape de X_test')\n",
    "print(X_test.shape)\n",
    "print(f'shape de y_train')\n",
    "print(y_train.shape)\n",
    "print (y_train)\n",
    "print(f'shape de y_test')\n",
    "print(y_test.shape)\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "\n",
    "#Entraînement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "##### Attention les fonctions en dessous ne marcheront pas car je n'ai pas sauvegardé l'historique\n",
    "# pour la prédiction il faut s'inspirer de y_hat = self.predict(X[nb_x])\n",
    "# pour l'instant predict attend un tableau, là il faut passer valeur par valeur\n",
    "#Prédiction\n",
    "for i in range (y_test.shape[0]):\n",
    "    y_pred=network.predict(X_test[i])\n",
    "    accuracy_test = network.accuracy(y_pred, y_test[i])\n",
    "    print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "#y_pred=network.predict(X_test)\n",
    "#accuracy_test = network.accuracy(y_pred, y_test)\n",
    "#print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "\n",
    "# Affichage des historiques\n",
    "#plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
