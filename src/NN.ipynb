{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.legend_handler import HandlerLine2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Déclarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories (eta,epochs,cost_history,accuracy_history):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs),cost_history,label='Cost')\n",
    "    line2, = ax.plot(range(epochs),accuracy_history,label='Accuracy')\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=4)})\n",
    "    \n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01,x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x<=0] = 0.01\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx / expx.sum(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,output,*args,**kwargs):\n",
    "        \n",
    "        self.output = output # Number of neurons at  layer i (current layer) \n",
    "        self.input = kwargs.get(\"input\",None) # Number of neurons at layer i-1\n",
    "        self.activ_function_curr = kwargs.get(\"activation\",None) # Activation function for the layer\n",
    "        self.parameters ={}\n",
    "        self.derivatives={}\n",
    "        self.activation_func=None\n",
    "        if self.activ_function_curr == \"relu\":\n",
    "            self.activation_func = relu\n",
    "            self.backward_activation_func = relu_prime\n",
    "        elif self.activ_function_curr == \"sigmoid\":\n",
    "            self.activation_func = sigmoid\n",
    "            self.backward_activation_func = sigmoid_prime\n",
    "        elif self.activ_function_curr == \"tanh\":\n",
    "            self.activation_func = tanh\n",
    "            self.backward_activation_func = tanh_prime\n",
    "        elif self.activ_function_curr == \"leakyrelu\":\n",
    "            self.activation_func = leakyrelu\n",
    "            self.backward_activation_func = leakyrelu_prime\n",
    "        elif self.activ_function_curr == \"softmax\":\n",
    "            self.activation_func = softmax\n",
    "            self.backward_activation_func = softmax    \n",
    "        \n",
    "    def initParams(self):\n",
    "        # initialisation du dictionnaire de données parameters contenant W, A et Z pour un layer\n",
    "        seed=30\n",
    "        np.random.seed(seed)\n",
    "        self.parameters['W']=np.random.randn(self.output,self.input)*np.sqrt(2/self.input)\n",
    "        self.parameters['b']=np.random.randn(self.output,1)*0.1\n",
    "               \n",
    "    def setW(self,matW):\n",
    "        self.parameters['W']=np.copy(matW)\n",
    "        \n",
    "    def setA(self,matA):\n",
    "        self.parameters['A']=np.copy(matA) \n",
    "        \n",
    "    def setZ(self,matZ):\n",
    "        self.parameters['Z']=np.copy(matZ)\n",
    "    \n",
    "    def setB(self,matB):\n",
    "        self.parameters['b']=np.copy(matB)\n",
    "        \n",
    "    def setdW(self,matdW):\n",
    "        self.parameters['dW']=np.copy(matdW)\n",
    "        \n",
    "    def setdA(self,matdA):\n",
    "        self.parameters['dA']=np.copy(matdA)\n",
    "        \n",
    "    def setdZ(self,matdZ):\n",
    "        self.parameters['dZ']=np.copy(matdZ)\n",
    "    \n",
    "    def setdB(self,matdB):\n",
    "        self.parameters['db']=np.copy(matdB)\n",
    "        \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers=0\n",
    "        self.layers=[]     \n",
    "        \n",
    "    def info(self):\n",
    "        print(\"Content of the network:\");\n",
    "        j=0;\n",
    "        for i in range(len(self.layers)):\n",
    "            print(\"Layer n° \",i,\" => \")\n",
    "            print (\"\\tInput \", self.layers[i].input, \n",
    "                   \"\\tOutput\", self.layers[i].output)             \n",
    "            if (i != 0):\n",
    "                print (\"\\tActivation Function\",self.layers[i].activation_func)\n",
    "                print (\"\\tW\", self.layers[i].parameters['W'].shape,self.layers[i].parameters['W'])\n",
    "                print (\"\\tb\", self.layers[i].parameters['b'].shape,self.layers[i].parameters['b'])\n",
    "\n",
    "    #ajout d'un param pour le type\n",
    "    def addLayer(self,layer):\n",
    "        self.nbLayers += 1;\n",
    "        if (self.nbLayers==1): \n",
    "            # this is the first layer so adding a layer 0\n",
    "            layerZero=Layer(layer.input)\n",
    "            self.layers.append(layerZero)\n",
    "            \n",
    "        self.layers.append(layer) \n",
    "        self.layers[self.nbLayers].input=self.layers[self.nbLayers-1].output\n",
    "        #self.layers[self.nbLayers].output=self.layers[self.nbLayers].output\n",
    "        layer.initParams()\n",
    "\n",
    "        \n",
    "        \n",
    "    def set_parametersW_b (self,numlayer,matX,matb):\n",
    "        self.layers[numlayer].parameters['W']=np.copy(matX)\n",
    "        self.layers[numlayer].parameters['b']=np.copy(matb)\n",
    "        \n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        #Init predictive variables for the input layer\n",
    "        self.layers[0].setA(X)\n",
    "        \n",
    "        #Propagation for all the layers\n",
    "        for l in range(1, self.nbLayers + 1):\n",
    "            # Compute Z\n",
    "            self.layers[l].setZ(np.dot(self.layers[l].parameters['W'],\n",
    "                                       self.layers[l-1].parameters['A'])+self.layers[l].parameters['b'])\n",
    "            # Applying the activation function of the layer to Z\n",
    "            self.layers[l].setA(self.layers[l].activation_func(self.layers[l].parameters['Z']))\n",
    "            \n",
    "    \n",
    "    def cost_function(self,y):            \n",
    "        return (-(y*np.log(self.layers[self.nbLayers].parameters['A']+1e-8) + (1-y)*np.log( 1 - self.layers[self.nbLayers].parameters['A']+1e-8))).mean()\n",
    "    \n",
    "    def backward_propagation(self,y):\n",
    "        #calcul de dZ dW et db pour le dernier layer\n",
    "        self.layers[self.nbLayers].derivatives['dZ']=self.layers[self.nbLayers].parameters['A']-y\n",
    "        self.layers[self.nbLayers].derivatives['dW']=np.dot(self.layers[self.nbLayers].derivatives['dZ'],\n",
    "                                                             np.transpose(self.layers[self.nbLayers-1].parameters['A']))\n",
    "        m=self.layers[self.nbLayers].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "        self.layers[self.nbLayers].derivatives['db']=np.sum(self.layers[self.nbLayers].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m\n",
    "        \n",
    "        #calcul de dZ dW db pour les autres layers\n",
    "        for l in range(self.nbLayers-1,0,-1) :\n",
    "            self.layers[l].derivatives['dZ']=np.dot(np.transpose(self.layers[l+1].parameters['W']),\n",
    "                                            self.layers[l+1].derivatives['dZ'])*self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "            \n",
    "            self.layers[l].derivatives[\"dW\"]=np.dot(self.layers[l].derivatives['dZ'],\n",
    "                                            np.transpose(self.layers[l-1].parameters['A']))\n",
    "                       \n",
    "            m=self.layers[l-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "            self.layers[l].derivatives['db']=np.sum(self.layers[l].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m    \n",
    "            \n",
    "    def update_parameters(self, eta) :\n",
    "        for l in range(1,self.nbLayers+1) :\n",
    "            self.layers[l].parameters['W']-=eta*self.layers[l].derivatives['dW']\n",
    "            self.layers[l].parameters[\"b\"]-=eta*self.layers[l].derivatives[\"db\"]\n",
    "            \n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "    \n",
    "    def plot_W_b_epoch (self,epoch,parameter_history):\n",
    "        mat=[]\n",
    "        max_size_layer=0\n",
    "        for l in range(1, self.nbLayers+1):    \n",
    "            value=parameter_history[epoch]['W'+str(l)]\n",
    "            if (parameter_history[epoch]['W'+str(l)].shape[1]>max_size_layer):\n",
    "                max_size_layer=parameter_history[epoch]['W'+str(l)].shape[1]\n",
    "            mat.append(value)\n",
    "        figure=plt.figure(figsize=((self.nbLayers+1)*3,int (max_size_layer/2)))    \n",
    "        for nb_w in range (len(mat)):    \n",
    "                plt.subplot(1, len(mat), nb_w+1)\n",
    "                plt.matshow(mat[nb_w],cmap = plt.cm.gist_rainbow,fignum=False, aspect='auto')\n",
    "                plt.colorbar()    \n",
    "        thelegend=\"Epoch \"+str(epoch)\n",
    "        plt.title (thelegend)    \n",
    "\n",
    "    def accuracy(self,y_hat, y):\n",
    "        if self.layers[self.nbLayers].activation_func==softmax:\n",
    "            # si la fonction est softmax, les valeurs sont sur différentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # où est la valeur maximale à la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded=np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis=0)\n",
    "            y_encoded=np.copy(y)\n",
    "            y_encoded=np.argmax(y_encoded, axis=0)\n",
    "            return (y_hat_encoded == y_encoded).mean()\n",
    "        # la dernière fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilité du résultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()       \n",
    "    \n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.layers[self.nbLayers].parameters['A']\n",
    "    \n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        batchsize=kwargs.get(\"batchsize\",32)\n",
    "    #def fit(self, X, y, epochs, eta = 0.01,batchsize=64) :\n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        parameter_history = []\n",
    "        for i in range(epochs):\n",
    "            i+=1\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost_batch = []\n",
    "            accuracy_batch = []\n",
    "            # Descente de gradient par mini-batch\n",
    "            for (batchX, batchy) in self.next_batch(X, y, batchsize):\n",
    "                # Extraction et traitement d'un batch à la fois\n",
    "                \n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                if self.layers[self.nbLayers].activation_func==softmax:\n",
    "                    # la classification n'est pas binaire, y a utilisé one-hot-encoder\n",
    "                    # le batchy doit donc être transposé et le résultat doit\n",
    "                    # être sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    \n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe\n",
    "                    # pas\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                #batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                self.forward_propagation(batchX)\n",
    "                self.backward_propagation(batchy)\n",
    "                self.update_parameters(eta)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                current_cost=self.cost_function(batchy)\n",
    "                cost_batch.append(current_cost)\n",
    "                y_hat = self.predict(batchX)\n",
    "                current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                accuracy_batch.append(current_accuracy)\n",
    "               \n",
    "            # SaveStats on W, B as well as values for A,Z, W, b\n",
    "            save_values = {}\n",
    "            save_values[\"epoch\"]=i\n",
    "            for l in range(1, self.nbLayers+1):\n",
    "                save_values[\"layer\"+str(l)]=l\n",
    "                save_values[\"Wmean\"+ str(l)]=np.mean(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmax\"+ str(l)]=np.amax(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmin\"+str(l)]=np.amin(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wstd\"+str(l)]=np.std(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"bmean\"+ str(l)]=np.mean(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmax\"+ str(l)]=np.amax(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmin\"+str(l)]=np.amin(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bstd\"+str(l)]=np.std(self.layers[self.nbLayers].parameters['b'])\n",
    "                # be careful A,Z,W and b must be copied otherwise it is a referencee\n",
    "                save_values['A'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['A'])\n",
    "                save_values['Z'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['Z'])\n",
    "                save_values['W'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values['b'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['b'])\n",
    "                \n",
    "            parameter_history.append(save_values)        \n",
    "            # sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost_batch)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy_batch)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(verbose == True):\n",
    "                print(\"Epoch : #%s/%s - %s/%s - cost : %.4f - accuracy : %.4f\"%(i,epochs,X.shape[1],X.shape[1], float(current_cost), current_accuracy))\n",
    "              \n",
    "        return self.layers, cost_history, accuracy_history, parameter_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['SepalLengthCm', 'SepalWidthCm', \n",
    "         'PetalLengthCm', 'PetalWidthCm', \n",
    "         'Species']\n",
    "\n",
    "df = pd.read_csv(url, names=names)\n",
    "\n",
    "# mélange des données\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "array = df.values #necessité de convertir le dataframe en numpy\n",
    "#X matrice de variables prédictives - attention forcer le type à float\n",
    "X = array[:,0:4].astype('float64') \n",
    "\n",
    "#y vecteur de variable à prédire \n",
    "y = array[:,4]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# normalisation de X\n",
    "sc_X=StandardScaler()\n",
    "X=sc_X.fit_transform(X)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Conversion de la variable à prédire via OneHotEncoder \n",
    "# Dans IRIS il y a 3 classes -> création de 3 colonnes pour y\n",
    "# 1 colonne correspond à 1 classe -> 1 si la ligne est du type de la classe\n",
    "# 0 sinon\n",
    "\n",
    "# Integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Jeu de test/apprentissage\n",
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "#transformation des données pour être au bon format\n",
    "# X_train est de la forme : n colonnes (variables à prédire après OneHotEncoder), m lignes (examples)\n",
    "# y_train est de la forme : m colonnes, n lignes (variables à prédire après OneHotEncoder)\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prédictives)\n",
    "X_train=np.transpose(X_train)\n",
    "\n",
    "# y_train est forcé pour être un tableau à 1 ligne contenant m colonnes\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], y_train.shape[1])))\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], y_test.shape[1])))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "network = NeuralNetwork()\n",
    "\n",
    "network.addLayer(Layer(10,input=4,activation=\"leakyrelu\"))\n",
    "network.addLayer(Layer(3,activation=\"softmax\"))\n",
    "\n",
    "network.info()\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "eta = 0.01\n",
    "batchsize=128\n",
    "\n",
    "#Entraînement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "\n",
    "#Prédiction\n",
    "y_pred=network.predict(X_test)\n",
    "accuracy_test = network.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['0','1','2','3','4','5','6','7','8','9','pred']\n",
    "\n",
    "df = pd.read_csv('csv/mnist.csv', names=names)\n",
    "\n",
    "# mélange des données\n",
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "array = df.values #necessité de convertir le dataframe en numpy\n",
    "#X matrice de variables prédictives - attention forcer le type à float\n",
    "X = array[:,0:10].astype('float64') \n",
    "#print(X)\n",
    "\n",
    "#y vecteur de variable à prédire \n",
    "y = array[:,10]\n",
    "#print(y)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# normalisation de X\n",
    "sc_X=StandardScaler()\n",
    "X=sc_X.fit_transform(X)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Conversion de la variable à prédire via OneHotEncoder \n",
    "# Dans IRIS il y a 3 classes -> création de 3 colonnes pour y\n",
    "# 1 colonne correspond à 1 classe -> 1 si la ligne est du type de la classe\n",
    "# 0 sinon\n",
    "\n",
    "# Integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Jeu de test/apprentissage\n",
    "validation_size=0.6 #40% du jeu de données pour le test\n",
    "\n",
    "testsize= 1-validation_size\n",
    "seed=30\n",
    "# séparation jeu d'apprentissage et jeu de test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, \n",
    "                                               y, \n",
    "                                               train_size=validation_size, \n",
    "                                               random_state=seed,\n",
    "                                               test_size=testsize)\n",
    "\n",
    "\n",
    "#transformation des données pour être au bon format\n",
    "# X_train est de la forme : n colonnes (variables à prédire après OneHotEncoder), m lignes (examples)\n",
    "# y_train est de la forme : m colonnes, n lignes (variables à prédire après OneHotEncoder)\n",
    "\n",
    "# La transposée de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prédictives)\n",
    "X_train=np.transpose(X_train)\n",
    "\n",
    "# y_train est forcé pour être un tableau à 1 ligne contenant m colonnes\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], y_train.shape[1])))\n",
    "\n",
    "# mêmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], y_test.shape[1])))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "network = NeuralNetwork()\n",
    "\n",
    "network.addLayer(Layer(20,input=10,activation=\"leakyrelu\"))\n",
    "network.addLayer(Layer(10,activation=\"softmax\"))\n",
    "\n",
    "#network.info()\n",
    "\n",
    "epochs = 20\n",
    "eta = 0.01\n",
    "batchsize=128\n",
    "\n",
    "#Entraînement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "\n",
    "#Prédiction\n",
    "y_pred=network.predict(X_test)\n",
    "print(\"y_pred.shape = \",y_pred.shape)\n",
    "print(\"y_test.shape = \",y_test.shape)\n",
    "accuracy_test = network.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test: %.3f\"%accuracy_test)\n",
    "\n",
    "# Affichage des historiques\n",
    "plot_histories (eta,epochs,cost_history,accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
