{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mnist\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        DÃ©clarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories (eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})\n",
    "\n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis = 0) - 0.1\n",
    "    amax, bmax = X.max(axis = 0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "\n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap = cm, alpha = 0.8)\n",
    "\n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y, cmap = cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01, x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x <= 0] = 0.01\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx / expx.sum(axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3: # A Convolution layer using 3x3 filters.\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "        # We divide by 9 to reduce the variance of our initial values\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(f'input : {input.shape}')\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        self.last_input = input\n",
    "        h, w = input.shape\n",
    "        output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        #print(f'output : {output.shape}')\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        '''\n",
    "        Performs a backward pass of the conv layer.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        - learn_rate is a float.\n",
    "        '''\n",
    "        d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "        # Update filters\n",
    "        self.filters -= learn_rate * d_L_d_filters\n",
    "\n",
    "        # We aren't returning anything here since we use Conv3x3 as the first layer in our CNN.\n",
    "        # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
    "        # other layer in our CNN.\n",
    "        return None\n",
    "\n",
    "class MaxPool2: # A Max Pooling layer using a pool size of 2.\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(f'input : {input.shape}')\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        #print(f'output : {output.shape}')\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        '''\n",
    "        Performs a backward pass of the maxpool layer.\n",
    "        Returns the loss gradient for this layer's inputs.\n",
    "        - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "        '''\n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        # If this pixel was the max value, copy the gradient to it.\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "        return d_L_d_input\n",
    "    \n",
    "class Flatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        #print(f'input : {input.shape}')\n",
    "        self.last_input_shape = input.shape\n",
    "        output = input.flatten()\n",
    "        self.last_input = output\n",
    "        #print(f'output : {output.shape}')\n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        return d_L_d_out.reshape(self.last_input_shape)\n",
    "    \n",
    "class MyLayer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.input = kwargs.get(\"input\", None) # Number of neurons at layer i-1\n",
    "        self.output = kwargs.get(\"output\", None) # Number of neurons at  layer i (current layer) \n",
    "        self.activ_function_curr = kwargs.get(\"activation\", None) # Activation function for the layer\n",
    "        self.paramCouche = kwargs.get(\"paramCouche\", None) # Param de la couche Conv3x3\n",
    "        self.type = kwargs.get(\"type\", None)\n",
    "        self.couche = kwargs.get(\"couche\", None)\n",
    "        self.parameters = {}\n",
    "        self.derivatives = {}\n",
    "        self.activation_func = None\n",
    "        self.activationCNNFunc = None\n",
    "        self.outputCNN = None\n",
    "        \n",
    "        if self.type == 'RNN':\n",
    "            if self.activ_function_curr == \"relu\":\n",
    "                self.activation_func = relu\n",
    "                self.backward_activation_func = relu_prime\n",
    "            elif self.activ_function_curr == \"sigmoid\":\n",
    "                self.activation_func = sigmoid\n",
    "                self.backward_activation_func = sigmoid_prime\n",
    "            elif self.activ_function_curr == \"tanh\":\n",
    "                self.activation_func = tanh\n",
    "                self.backward_activation_func = tanh_prime\n",
    "            elif self.activ_function_curr == \"leakyrelu\":\n",
    "                self.activation_func = leakyrelu\n",
    "                self.backward_activation_func = leakyrelu_prime\n",
    "            elif self.activ_function_curr == \"softmax\":\n",
    "                self.activation_func = softmax\n",
    "                self.backward_activation_func = softmax\n",
    "        elif self.type == 'CNN':\n",
    "            if self.couche == \"Conv3x3\":\n",
    "                self.outputCNN = Conv3x3(int(self.paramCouche))\n",
    "                self.activationCNNFunc = self.outputCNN.forward\n",
    "                self.backwardactivationCNNFunc = self.outputCNN.backprop\n",
    "            elif self.couche == \"MaxPool2\":\n",
    "                self.outputCNN = MaxPool2()\n",
    "                self.activationCNNFunc = self.outputCNN.forward\n",
    "                self.backwardactivationCNNFunc = self.outputCNN.backprop\n",
    "            elif self.couche == \"flatten\":\n",
    "                self.outputCNN = Flatten()\n",
    "                self.activationCNNFunc = self.outputCNN.forward\n",
    "                self.backwardactivationCNNFunc = self.outputCNN.backprop\n",
    "                \n",
    "    def initParams(self):\n",
    "        # Initialisation du dictionnaire de donnÃ©es parameters contenant W, A et Z pour un layer\n",
    "        seed = 30\n",
    "        np.random.seed(seed)\n",
    "        self.parameters['W'] = np.random.randn(self.output, self.input) * np.sqrt(2 / self.input)\n",
    "        self.parameters['b'] = np.random.randn(self.output, 1) * 0.1\n",
    "\n",
    "    def setW(self, matW):\n",
    "        self.parameters['W'] = np.copy(matW)\n",
    "        \n",
    "    def setA(self, matA):\n",
    "        self.parameters['A'] = np.copy(matA) \n",
    "        \n",
    "    def setZ(self, matZ):\n",
    "        self.parameters['Z'] = np.copy(matZ)\n",
    "    \n",
    "    def setB(self, matB):\n",
    "        self.parameters['b'] = np.copy(matB)\n",
    "        \n",
    "    def setdW(self, matdW):\n",
    "        self.parameters['dW'] = np.copy(matdW)\n",
    "        \n",
    "    def setdA(self, matdA):\n",
    "        self.parameters['dA'] = np.copy(matdA)\n",
    "        \n",
    "    def setdZ(self, matdZ):\n",
    "        self.parameters['dZ'] = np.copy(matdZ)\n",
    "    \n",
    "    def setdB(self, matdB):\n",
    "        self.parameters['db'] = np.copy(matdB)\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.nbCNNlayers = 0\n",
    "        self.layers = [] # NN layers\n",
    "        self.CNN = [] # CNN layers\n",
    "\n",
    "    def printLayers(self):\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(self.CNN[i].activationCNNFunc)\n",
    "        for i in range(len(self.layers)):\n",
    "            print(self.layers[i].activation_func)\n",
    "        \n",
    "    def info(self):\n",
    "        print(f'Content of the network :');\n",
    "        j = 0;\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(f'\\n\\tLayer nÂ° {i} du CNN => ')\n",
    "            print(f'\\t\\tInput : {self.CNN[i].input}\\n\\t\\tOutput : {self.CNN[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.CNN[i].activationCNNFunc}')\n",
    "                print(f'\\t\\tW shape : {self.CNN[i].parameters[\"W\"].shape}\\n\\t\\tW data :\\n{self.CNN[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.CNN[i].parameters[\"b\"].shape}\\n\\t\\tb data :\\n{self.CNN[i].parameters[\"b\"]}')\n",
    "                \n",
    "        for i in range(len(self.layers)):\n",
    "            print(f'\\n\\tLayer nÂ° {i} du NN => ')\n",
    "            print(f'\\t\\tInput : {self.layers[i].input}\\n\\t\\tOutput : {self.layers[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "                print(f'\\t\\tW shape : {self.layers[i].parameters[\"W\"].shape}\\n\\t\\tW data :\\n{self.layers[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.layers[i].parameters[\"b\"].shape}\\n\\t\\tb data :\\n{self.layers[i].parameters[\"b\"]}')\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        if(layer.type == 'CNN'):\n",
    "            self.nbCNNlayers += 1\n",
    "            if(self.nbCNNlayers == 1):\n",
    "                layerZeroCNN = MyLayer(output = layer.input)\n",
    "                self.CNN.append(layerZeroCNN)\n",
    "            self.CNN.append(layer)\n",
    "            self.CNN[self.nbCNNlayers].input = self.CNN[self.nbCNNlayers - 1].output\n",
    "            layer.initParams()\n",
    "        elif(layer.type == 'RNN'):\n",
    "            self.nbLayers += 1;\n",
    "            if(self.nbLayers == 1):\n",
    "                layer.input = self.CNN[-1].output\n",
    "                layerZeroNN = MyLayer(output = layer.input)\n",
    "                self.layers.append(layerZeroNN)\n",
    "                self.layers.append(layer)\n",
    "                layer.initParams()\n",
    "            else:\n",
    "                self.layers.append(layer)\n",
    "                self.layers[self.nbLayers].input = self.layers[self.nbLayers - 1].output\n",
    "                layer.initParams()\n",
    "\n",
    "    def set_parametersW_b (self, numlayer, matX, matb):\n",
    "        self.layers[numlayer].parameters['W'] = np.copy(matX)\n",
    "        self.layers[numlayer].parameters['b'] = np.copy(matb)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        for i in range(X.shape[2]):\n",
    "            justInCaseOutputCNN = self.CNN[1].activationCNNFunc(X[:, :, i])\n",
    "            #print(f'FORWARD CNN : input : {self.CNN[1].input}')\n",
    "            #print(f'FORWARD CNN : output : {self.CNN[1].output}')\n",
    "            for l in range(2, self.nbCNNlayers + 1):\n",
    "                #print(f'FORWARD CNN : input : {self.CNN[l].input}')\n",
    "                #print(f'FORWARD CNN : output : {self.CNN[l].output}')\n",
    "                justInCaseOutputCNN = self.CNN[l].activationCNNFunc(justInCaseOutputCNN)\n",
    "\n",
    "            self.layers[0].setA(justInCaseOutputCNN)\n",
    "\n",
    "            for l in range(1, self.nbLayers + 1):\n",
    "                #print(f'FORWARD RRRRRRRRRRRRRRRRRRRRRNN : input : {self.layers[l].input}')\n",
    "                #print(f'FORWARD RRRRRRRRRRRRRRRRRRRRRNN : output : {self.layers[l].output}')\n",
    "                # Compute Z\n",
    "                self.layers[l].setZ(np.dot(self.layers[l].parameters['W'], self.layers[l - 1].parameters['A']) + self.layers[l].parameters['b'])\n",
    "                # Applying the activation function of the layer to Z\n",
    "                self.layers[l].setA(self.layers[l].activation_func(self.layers[l].parameters['Z']))\n",
    "\n",
    "    def cost_function(self, y):\n",
    "        return (-(y * np.log(self.layers[self.nbLayers].parameters['A'] + 1e-8) + (1 - y) * np.log(1 - self.layers[self.nbLayers].parameters['A'] + 1e-8))).mean()\n",
    "\n",
    "    def backward_propagation(self, y):\n",
    "        # Calcul de dZ dW et db pour le dernier layer\n",
    "        self.layers[self.nbLayers].derivatives['dZ'] = self.layers[self.nbLayers].parameters['A'] - y\n",
    "        self.layers[self.nbLayers].derivatives['dW'] = np.dot(self.layers[self.nbLayers].derivatives['dZ'], np.transpose(self.layers[self.nbLayers - 1].parameters['A']))\n",
    "        m = self.layers[self.nbLayers].parameters['A'].shape[1] # Ã©gal au nombre de colonnes de A \n",
    "        self.layers[self.nbLayers].derivatives['db'] = np.sum(self.layers[self.nbLayers].derivatives['dZ'], axis = 1, keepdims = True) / m\n",
    "        \n",
    "        # Calcul de dZ dW db pour les autres layers\n",
    "        for l in range(self.nbLayers - 1, 0, -1) :\n",
    "            self.layers[l].derivatives['dZ'] = np.dot(np.transpose(self.layers[l + 1].parameters['W']), self.layers[l + 1].derivatives['dZ']) * self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "            self.layers[l].derivatives[\"dW\"] = np.dot(self.layers[l].derivatives['dZ'], np.transpose(self.layers[l - 1].parameters['A']))\n",
    "            m = self.layers[l - 1].parameters['A'].shape[1] # Ã©gal au nombre de colonnes de A \n",
    "            self.layers[l].derivatives['db'] = np.sum(self.layers[l].derivatives['dZ'], axis = 1, keepdims = True) / m    \n",
    "                      \n",
    "        outputBackprop = self.CNN[self.nbCNNlayers].backward_activation_func(self.layers[0].derivatives)\n",
    "\n",
    "        for l in range(self.nbCNNlayers - 1, 1, -1) :\n",
    "            outputBackprop = self.CNN[l].backward_activation_func(outputBackprop)\n",
    "                      \n",
    "    def update_parameters(self, eta) :\n",
    "        for l in range(1, self.nbLayers + 1) :\n",
    "            self.layers[l].parameters['W'] -= eta * self.layers[l].derivatives['dW']\n",
    "            self.layers[l].parameters[\"b\"] -= eta * self.layers[l].derivatives[\"db\"]\n",
    "\n",
    "    def convert_prob_into_class(self, probs):\n",
    "        probs = np.copy(probs) # Pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "    def plot_W_b_epoch(self, epoch, parameter_history):\n",
    "        mat = []\n",
    "        max_size_layer = 0\n",
    "        for l in range(1, self.nbLayers + 1):    \n",
    "            value = parameter_history[epoch]['W' + str(l)]\n",
    "            if(parameter_history[epoch]['W' + str(l)].shape[1] > max_size_layer):\n",
    "                max_size_layer = parameter_history[epoch]['W' + str(l)].shape[1]\n",
    "            mat.append(value)\n",
    "        figure = plt.figure(figsize = ((self.nbLayers + 1) * 3, int(max_size_layer / 2)))\n",
    "        for nb_w in range(len(mat)):\n",
    "                plt.subplot(1, len(mat), nb_w + 1)\n",
    "                plt.matshow(mat[nb_w], cmap = plt.cm.gist_rainbow, fignum = False, aspect = 'auto')\n",
    "                plt.colorbar()\n",
    "        thelegend = \"Epoch \" + str(epoch)\n",
    "        plt.title(thelegend)    \n",
    "\n",
    "    def accuracy(self, y_hat, y):\n",
    "        if self.layers[self.nbLayers].activation_func == softmax:\n",
    "            # si la fonction est softmax, les valeurs sont sur diffÃ©rentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # oÃ¹ est la valeur maximale Ã  la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded = np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis = 0)\n",
    "            y_encoded = np.copy(y)\n",
    "            y_encoded = np.argmax(y_encoded, axis = 0)\n",
    "            return(y_hat_encoded == y_encoded).mean()\n",
    "        # la derniÃ¨re fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilitÃ© du rÃ©sultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return(y_hat_ == y).all(axis = 0).mean()\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        print(f'predict params A : {self.layers[self.nbLayers].parameters[\"A\"].shape}')\n",
    "        return self.layers[self.nbLayers].parameters['A']\n",
    "\n",
    "    def next_batch(self, X, y, batchsize):\n",
    "        # Pour avoir X de la forme : 2 colonnes, m lignes (examples) et Ã©galement y\n",
    "        # Cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X = np.transpose(X)\n",
    "        y = np.transpose(y)\n",
    "        \n",
    "        m = len(y)\n",
    "        # Permutation alÃ©atoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # Creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "\n",
    "    def fit(self, X, y, *args, **kwargs):\n",
    "        epochs = kwargs.get(\"epochs\", 20)\n",
    "        verbose = kwargs.get(\"verbose\", False)\n",
    "        eta = kwargs.get(\"eta\", 0.01)\n",
    "        batchsize = kwargs.get(\"batchsize\", 32)\n",
    "\n",
    "        # Sauvegarde historique coÃ»t et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        parameter_history = []\n",
    "        for i in range(epochs):\n",
    "            i += 1\n",
    "            # Sauvegarde des coÃ»ts et accuracy par mini-batch\n",
    "            cost_batch = []\n",
    "            accuracy_batch = []\n",
    "            # Descente de gradient par mini-batch\n",
    "            for(batchX, batchy) in self.next_batch(X, y, batchsize):\n",
    "                # Extraction et traitement d'un batch Ã  la fois\n",
    "                # Mise en place des donnÃ©es au bon format\n",
    "                batchX = np.transpose(batchX)\n",
    "                if self.layers[self.nbLayers].activation_func == softmax:\n",
    "                    # la classification n'est pas binaire, y a utilisÃ© one-hot-encoder\n",
    "                    # le batchy doit donc Ãªtre transposÃ© et le rÃ©sultat doit\n",
    "                    # Ãªtre sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    batchy = np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe pas\n",
    "                    batchy = np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                #batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                self.forward_propagation(batchX)\n",
    "                #self.backward_propagation(batchy)\n",
    "                #self.update_parameters(eta)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                #current_cost = self.cost_function(batchy)\n",
    "                #cost_batch.append(current_cost)\n",
    "                #y_hat = self.predict(batchX)\n",
    "                #current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                #accuracy_batch.append(current_accuracy)\n",
    "               \n",
    "            # SaveStats on W, B as well as values for A,Z, W, b\n",
    "            save_values = {}\n",
    "            save_values[\"epoch\"] = i\n",
    "            for l in range(1, self.nbLayers + 1):\n",
    "                save_values[\"layer\" + str(l)] = l\n",
    "                save_values[\"Wmean\" + str(l)] = np.mean(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmax\" + str(l)] = np.amax(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmin\" + str(l)] = np.amin(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wstd\" + str(l)] = np.std(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"bmean\" + str(l)] = np.mean(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmax\" + str(l)] = np.amax(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmin\" + str(l)] = np.amin(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bstd\" + str(l)] = np.std(self.layers[self.nbLayers].parameters['b'])\n",
    "                      \n",
    "                # Be careful A, Z, W and b must be copied otherwise it is a referencee\n",
    "                save_values['A' + str(l)] = np.copy(self.layers[self.nbLayers].parameters['A'])\n",
    "                save_values['Z' + str(l)] = np.copy(self.layers[self.nbLayers].parameters['Z'])\n",
    "                save_values['W' + str(l)] = np.copy(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values['b' + str(l)] = np.copy(self.layers[self.nbLayers].parameters['b'])\n",
    "                \n",
    "            parameter_history.append(save_values)   \n",
    "\n",
    "            # Sauvegarde de la valeur moyenne des coÃ»ts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost_batch)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy_batch)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(verbose == True):\n",
    "                print(f'Epoch : {i}/{epochs} | cost : {float(current_cost)} | accuracy : {current_accuracy}')\n",
    "              \n",
    "        return self.layers, cost_history, accuracy_history, parameter_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Applications\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Create a random network </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myNetwork = MyNeuralNetwork()\n",
    "\n",
    "myNetwork.addLayer(MyLayer(type = \"CNN\", input = 4, output = 10, couche = \"Conv3x3\", paramCouche = \"8\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"CNN\", output = 3, couche = \"MaxPool2\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"CNN\", output = 2, couche = \"flatten\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"RNN\", output = 10, activation = \"relu\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"RNN\", output = 5, activation = \"softmax\"))\n",
    "\n",
    "myNetwork.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classification of mnist data </h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atemp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = mnist.test_images()[:100]\n",
    "test_labels = mnist.test_labels()[:100]\n",
    "\n",
    "newTestImages = []\n",
    "for image in test_images:\n",
    "    newTestImages.append(((image / 255) - 0.5))\n",
    "\n",
    "X = np.float64(newTestImages)\n",
    "y = np.float64(test_labels)\n",
    "\n",
    "# Integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "# Jeu d'apprentissage 60%\n",
    "validation_size = 0.6\n",
    "\n",
    "# 40% du jeu de donnÃ©es pour le test\n",
    "testsize = 1 - validation_size\n",
    "\n",
    "seed = 30\n",
    "\n",
    "# sÃ©paration jeu d'apprentissage et jeu de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = validation_size, random_state = seed, test_size = testsize)\n",
    "\n",
    "# La transposÃ©e de X_train est de la forme : m colonnes (exemples), n lignes (nombre de variables prÃ©dictives)\n",
    "X_train=np.transpose(X_train)\n",
    "\n",
    "# y_train est forcÃ© pour Ãªtre un tableau Ã  1 ligne contenant m colonnes\n",
    "y_train=np.transpose(y_train.reshape((y_train.shape[0], y_train.shape[1])))\n",
    "\n",
    "# mÃªmes traitements pour le jeu de test\n",
    "X_test=np.transpose(X_test)\n",
    "y_test=np.transpose(y_test.reshape((y_test.shape[0], y_test.shape[1])))\n",
    "\n",
    "network = MyNeuralNetwork()\n",
    "\n",
    "network.addLayer(MyLayer(type = \"CNN\", input = 784, output = 5408, couche = \"Conv3x3\", paramCouche = \"8\"))\n",
    "network.addLayer(MyLayer(type = \"CNN\", output = 1352, couche = \"MaxPool2\"))\n",
    "network.addLayer(MyLayer(type = \"CNN\", output = 1352, couche = \"flatten\"))\n",
    "network.addLayer(MyLayer(type = \"RNN\", output = 1352, activation = \"relu\"))\n",
    "network.addLayer(MyLayer(type = \"RNN\", output = 10, activation = \"softmax\"))\n",
    "\n",
    "#network.info()\n",
    "\n",
    "epochs = 5\n",
    "eta = 0.01\n",
    "batchsize=128\n",
    "\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "print(f'shape de X_train')\n",
    "print(X_train.shape)\n",
    "print(f'shape de X_test')\n",
    "print(X_test.shape)\n",
    "print(f'shape de y_train')\n",
    "print(y_train.shape)\n",
    "print(f'shape de y_test')\n",
    "print(y_test.shape)\n",
    "print(f'-------------------------------------------------------------------------')\n",
    "\n",
    "#EntraÃ®nement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "#PrÃ©diction\n",
    "y_pred = network.predict(X_test)\n",
    "print(f'y_test {y_test.shape}')\n",
    "print(f'y_pred {y_pred.shape}')\n",
    "#accuracy_test = network.accuracy(y_pred, y_test)\n",
    "#print(f'Accuracy test : {accuracy_test}')\n",
    "\n",
    "# Affichage des historiques\n",
    "#plot_histories(eta,epochs,cost_history,accuracy_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
