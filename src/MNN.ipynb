{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Imports\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" align=\"center\">\n",
    "    <h1>\n",
    "        Déclarations\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Fonctions utiles </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories (eta, epochs, cost_history, accuracy_history):\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    ax.set_ylabel(r'$J(\\theta)$')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_title(r\"$\\eta$ :{}\".format(eta))\n",
    "    line1, = ax.plot(range(epochs), cost_history, label = 'Cost')\n",
    "    line2, = ax.plot(range(epochs), accuracy_history, label = 'Accuracy')\n",
    "    plt.legend(handler_map = {line1: HandlerLine2D(numpoints = 4)})\n",
    "\n",
    "def plot_decision_boundary(func, X, y):\n",
    "    amin, bmin = X.min(axis = 0) - 0.1\n",
    "    amax, bmax = X.max(axis = 0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "\n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    contour = plt.contourf(aa, bb, cc, cmap = cm, alpha = 0.8)\n",
    "\n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c = y, cmap = cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    x[x <= 0] = 0\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def leakyrelu(x):\n",
    "    return np.maximum(0.01, x)\n",
    "\n",
    "def leakyrelu_prime(x):\n",
    "    x[x <= 0] = 0.01\n",
    "    x[x > 0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx / expx.sum(axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Classes </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3: # A Convolution layer using 3x3 filters\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "        # We divide by 9 to reduce the variance of our initial values\n",
    "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates all possible 3x3 image regions using valid padding.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w = image.shape\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:(i + 3), j:(j + 3)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the conv layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "        - input is a 2d numpy array\n",
    "        '''\n",
    "        h, w = input.shape\n",
    "        output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "        return output\n",
    "\n",
    "class MaxPool2: # A Max Pooling layer using a pool size of 2\n",
    "    def iterate_regions(self, image):\n",
    "        '''\n",
    "        Generates non-overlapping 2x2 image regions to pool over.\n",
    "        - image is a 2d numpy array\n",
    "        '''\n",
    "        h, w, _ = image.shape\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of the maxpool layer using the given input.\n",
    "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "        '''\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Flatten: # A Flattening layer\n",
    "    def forward(self, input):\n",
    "        return input.flatten()\n",
    "    \n",
    "class MyLayer:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.input = kwargs.get(\"input\", None) # Number of neurons at layer i-1\n",
    "        self.output = kwargs.get(\"output\", None) # Number of neurons at  layer i (current layer) \n",
    "        self.activ_function_curr = kwargs.get(\"activation\", None) # Activation function for the layer\n",
    "        self.paramCouche = kwargs.get(\"paramCouche\", None) # Param de la couche Conv3x3\n",
    "        self.type = kwargs.get(\"type\", None)\n",
    "        self.couche = kwargs.get(\"couche\", None)\n",
    "        self.parameters = {}\n",
    "        self.derivatives = {}\n",
    "        self.activation_func = None\n",
    "        self.activationCNNFunc = None\n",
    "        self.outputCNN = None\n",
    "        \n",
    "        if self.type == 'RNN':\n",
    "            if self.activ_function_curr == \"relu\":\n",
    "                self.activation_func = relu\n",
    "                self.backward_activation_func = relu_prime\n",
    "            elif self.activ_function_curr == \"sigmoid\":\n",
    "                self.activation_func = sigmoid\n",
    "                self.backward_activation_func = sigmoid_prime\n",
    "            elif self.activ_function_curr == \"tanh\":\n",
    "                self.activation_func = tanh\n",
    "                self.backward_activation_func = tanh_prime\n",
    "            elif self.activ_function_curr == \"leakyrelu\":\n",
    "                self.activation_func = leakyrelu\n",
    "                self.backward_activation_func = leakyrelu_prime\n",
    "            elif self.activ_function_curr == \"softmax\":\n",
    "                self.activation_func = softmax\n",
    "                self.backward_activation_func = softmax\n",
    "        elif self.type == 'CNN':\n",
    "            if self.couche == \"Conv3x3\":\n",
    "                self.outputCNN = Conv3x3(int(self.paramCouche))\n",
    "                self.activationCNNFunc = self.outputCNN.forward\n",
    "                #self.backwardactivationCNNFunc = conv.backward()\n",
    "            elif self.couche == \"MaxPool2\":\n",
    "                self.outputCNN = MaxPool2()\n",
    "                self.activationCNNFunc = self.outputCNN.forward\n",
    "                #self.backwardactivationCNNFunc = maxPool.backward()\n",
    "            elif self.couche == \"flatten\":\n",
    "                self.outputCNN = Flatten()\n",
    "                self.activationCNNFunc = self.outputCNN.forward\n",
    "                #self.backwardactivationCNNFunc = flatten.backward()\n",
    "\n",
    "    def initParams(self):\n",
    "        # Initialisation du dictionnaire de données parameters contenant W, A et Z pour un layer\n",
    "        seed = 30\n",
    "        np.random.seed(seed)\n",
    "        self.parameters['W'] = np.random.randn(self.output, self.input) * np.sqrt(2 / self.input)\n",
    "        self.parameters['b'] = np.random.randn(self.output, 1) * 0.1\n",
    "\n",
    "    def setW(self, matW):\n",
    "        self.parameters['W'] = np.copy(matW)\n",
    "        \n",
    "    def setA(self, matA):\n",
    "        self.parameters['A'] = np.copy(matA) \n",
    "        \n",
    "    def setZ(self, matZ):\n",
    "        self.parameters['Z'] = np.copy(matZ)\n",
    "    \n",
    "    def setB(self, matB):\n",
    "        self.parameters['b'] = np.copy(matB)\n",
    "        \n",
    "    def setdW(self, matdW):\n",
    "        self.parameters['dW'] = np.copy(matdW)\n",
    "        \n",
    "    def setdA(self, matdA):\n",
    "        self.parameters['dA'] = np.copy(matdA)\n",
    "        \n",
    "    def setdZ(self, matdZ):\n",
    "        self.parameters['dZ'] = np.copy(matdZ)\n",
    "    \n",
    "    def setdB(self, matdB):\n",
    "        self.parameters['db'] = np.copy(matdB)\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.nbLayers = 0\n",
    "        self.nbCNNlayers = 0\n",
    "        self.layers = [] # NN layers\n",
    "        self.CNN = [] # CNN layers\n",
    "        \n",
    "    def info(self):\n",
    "        print(f'Content of the network :');\n",
    "        j = 0;\n",
    "        for i in range(len(self.CNN)):\n",
    "            print(f'\\n\\tLayer n° {i} du CNN => ')\n",
    "            print(f'\\t\\tInput : {self.CNN[i].input}\\n\\t\\tOutput : {self.CNN[i].output}')\n",
    "            if (i != 0):\n",
    "                print(f'\\t\\tCouche : {self.CNN[i].activationCNNFunc}')\n",
    "                print(f'\\t\\tW shape : {self.CNN[i].parameters[\"W\"].shape}\\n\\t\\tW data :\\n{self.CNN[i].parameters[\"W\"]}')\n",
    "                print(f'\\t\\tb shape : {self.CNN[i].parameters[\"b\"].shape}\\n\\t\\tb data :\\n{self.CNN[i].parameters[\"b\"]}')\n",
    "                \n",
    "        for i in range(len(self.layers)):\n",
    "            print(f'\\n\\tLayer n° {i} du NN => ')\n",
    "            print(f'\\t\\tInput : {self.layers[i].input}\\n\\t\\tOutput : {self.layers[i].output}')\n",
    "            print(f'\\t\\tCouche : {self.layers[i].activation_func}')\n",
    "            print(f'\\t\\tW shape : {self.layers[i].parameters[\"W\"].shape}\\n\\t\\tW data :\\n{self.layers[i].parameters[\"W\"]}')\n",
    "            print(f'\\t\\tb shape : {self.layers[i].parameters[\"b\"].shape}\\n\\t\\tb data :\\n{self.layers[i].parameters[\"b\"]}')\n",
    "\n",
    "    #TODO Ajout d'un param pour le type\n",
    "    def addLayer(self, layer):\n",
    "    #TODO Check l'indice du layer\n",
    "        if(layer.type == 'CNN'):\n",
    "            self.nbCNNlayers += 1\n",
    "            if(self.nbCNNlayers == 1):\n",
    "                layerZeroCNN = MyLayer(output = layer.input)\n",
    "                self.CNN.append(layerZeroCNN)\n",
    "            self.CNN.append(layer)\n",
    "            self.CNN[self.nbCNNlayers].input = self.CNN[self.nbCNNlayers - 1].output\n",
    "            layer.initParams()\n",
    "        elif(layer.type == 'RNN'):\n",
    "            self.nbLayers += 1;\n",
    "            if(self.nbLayers == 1):\n",
    "                layer.input = self.CNN[-1].output\n",
    "                self.layers.append(layer)\n",
    "                layer.initParams()\n",
    "            else:\n",
    "                self.layers.append(layer)\n",
    "                self.layers[self.nbLayers - 1].input = self.layers[self.nbLayers - 2].output\n",
    "                layer.initParams()\n",
    "\n",
    "    def set_parametersW_b (self, numlayer, matX, matb):\n",
    "        self.layers[numlayer].parameters['W'] = np.copy(matX)\n",
    "        self.layers[numlayer].parameters['b'] = np.copy(matb)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        #TODO Check la variable type et l'indice des layers pour le CNN & NN\n",
    "        if(type == 'CNN'):\n",
    "            # Init predictive variables for the input layer\n",
    "            self.CNN[0].setA(X)\n",
    "        elif(type=='RNN'):\n",
    "            self.layers[0].setA(self.CNN[-1].output)\n",
    "\n",
    "        # Propagation for all the layers\n",
    "        for l in range(1, self.nbCNNlayers + 1):\n",
    "            # Compute Z\n",
    "            self.CNN[l].setZ(np.dot(self.CNN[l].parameters['W'], self.CNN[l - 1].parameters['A']) + self.CNN[l].parameters['b'])\n",
    "            # Applying the activation function of the layer to Z\n",
    "            self.CNN[l].setA(self.CNN[l].activationCNNFunc(self.CNN[l].parameters['Z']))\n",
    "            \n",
    "        for l in range(0, self.nbLayers + 1):\n",
    "            # Compute Z\n",
    "            self.layers[l].setZ(np.dot(self.layers[l].parameters['W'], self.layers[l - 1].parameters['A']) + self.layers[l].parameters['b'])\n",
    "            # Applying the activation function of the layer to Z\n",
    "            self.layers[l].setA(self.layers[l].activation_func(self.layers[l].parameters['Z']))\n",
    "\n",
    "    def cost_function(self, y):\n",
    "        return (-(y * np.log(self.layers[self.nbLayers].parameters['A'] + 1e-8) + (1 - y) * np.log(1 - self.layers[self.nbLayers].parameters['A'] + 1e-8))).mean()\n",
    "\n",
    "    def backward_propagation(self, y):\n",
    "        #calcul de dZ dW et db pour le dernier layer\n",
    "        self.layers[self.nbLayers].derivatives['dZ']=self.layers[self.nbLayers].parameters['A']-y\n",
    "        self.layers[self.nbLayers].derivatives['dW']=np.dot(self.layers[self.nbLayers].derivatives['dZ'],\n",
    "                                                             np.transpose(self.layers[self.nbLayers-1].parameters['A']))\n",
    "        m=self.layers[self.nbLayers].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "        self.layers[self.nbLayers].derivatives['db']=np.sum(self.layers[self.nbLayers].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m\n",
    "        \n",
    "        #calcul de dZ dW db pour les autres layers\n",
    "        for l in range(self.nbLayers-1,0,-1) :\n",
    "            self.layers[l].derivatives['dZ']=np.dot(np.transpose(self.layers[l+1].parameters['W']),\n",
    "                                            self.layers[l+1].derivatives['dZ'])*self.layers[l].backward_activation_func(self.layers[l].parameters[\"Z\"])\n",
    "            \n",
    "            self.layers[l].derivatives[\"dW\"]=np.dot(self.layers[l].derivatives['dZ'],\n",
    "                                            np.transpose(self.layers[l-1].parameters['A']))\n",
    "                       \n",
    "            m=self.layers[l-1].parameters['A'].shape[1]#égal au nombre de colonnes de A \n",
    "            self.layers[l].derivatives['db']=np.sum(self.layers[l].derivatives['dZ'], \n",
    "                                                       axis=1, keepdims=True) / m    \n",
    "\n",
    "    def update_parameters(self, eta) :\n",
    "        for l in range(1,self.nbLayers+1) :\n",
    "            self.layers[l].parameters['W']-=eta*self.layers[l].derivatives['dW']\n",
    "            self.layers[l].parameters[\"b\"]-=eta*self.layers[l].derivatives[\"db\"]\n",
    "\n",
    "    def convert_prob_into_class(self,probs):\n",
    "        probs = np.copy(probs)#pour ne pas perdre probs, i.e. y_hat\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "        return probs\n",
    "\n",
    "    def plot_W_b_epoch (self,epoch,parameter_history):\n",
    "        mat=[]\n",
    "        max_size_layer=0\n",
    "        for l in range(1, self.nbLayers+1):    \n",
    "            value=parameter_history[epoch]['W'+str(l)]\n",
    "            if (parameter_history[epoch]['W'+str(l)].shape[1]>max_size_layer):\n",
    "                max_size_layer=parameter_history[epoch]['W'+str(l)].shape[1]\n",
    "            mat.append(value)\n",
    "        figure=plt.figure(figsize=((self.nbLayers+1)*3,int (max_size_layer/2)))    \n",
    "        for nb_w in range (len(mat)):    \n",
    "                plt.subplot(1, len(mat), nb_w+1)\n",
    "                plt.matshow(mat[nb_w],cmap = plt.cm.gist_rainbow,fignum=False, aspect='auto')\n",
    "                plt.colorbar()    \n",
    "        thelegend=\"Epoch \"+str(epoch)\n",
    "        plt.title (thelegend)    \n",
    "\n",
    "    def accuracy(self,y_hat, y):\n",
    "        if self.layers[self.nbLayers].activation_func==softmax:\n",
    "            # si la fonction est softmax, les valeurs sont sur différentes dimensions\n",
    "            # il faut utiliser argmax avec axis=0 pour avoir un vecteur qui indique\n",
    "            # où est la valeur maximale à la fois pour y_hat et pour y\n",
    "            # comme cela il suffit de comparer les deux vecteurs qui indiquent \n",
    "            # dans quelle ligne se trouve le max\n",
    "            y_hat_encoded=np.copy(y_hat)\n",
    "            y_hat_encoded = np.argmax(y_hat_encoded, axis=0)\n",
    "            y_encoded=np.copy(y)\n",
    "            y_encoded=np.argmax(y_encoded, axis=0)\n",
    "            return (y_hat_encoded == y_encoded).mean()\n",
    "        # la dernière fonction d'activation n'est pas softmax.\n",
    "        # par exemple sigmoid pour une classification binaire\n",
    "        # il suffit de convertir la probabilité du résultat en classe\n",
    "        y_hat_ = self.convert_prob_into_class(y_hat)\n",
    "        return (y_hat_ == y).all(axis=0).mean()       \n",
    "\n",
    "    def predict(self, x):\n",
    "        self.forward_propagation(x)\n",
    "        return self.layers[self.nbLayers].parameters['A']\n",
    "\n",
    "    def next_batch(self,X, y, batchsize):\n",
    "        # pour avoir X de la forme : 2 colonnes, m lignes (examples) et également y\n",
    "        # cela permet de trier les 2 tableaux avec un indices de permutation       \n",
    "        X=np.transpose(X)\n",
    "        y=np.transpose(y)\n",
    "        \n",
    "        m=len(y)\n",
    "        # permutation aléatoire de X et y pour faire des batchs avec des valeurs au hasard\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in np.arange(0, X.shape[0], batchsize):\n",
    "            # creation des batchs de taille batchsize\n",
    "            yield (X[i:i + batchsize], y[i:i + batchsize])\n",
    "\n",
    "    def fit(self, X, y, *args,**kwargs):    \n",
    "        epochs=kwargs.get(\"epochs\",20)\n",
    "        verbose=kwargs.get(\"verbose\",False)\n",
    "        eta =kwargs.get(\"eta\",0.01)\n",
    "        batchsize=kwargs.get(\"batchsize\",32)\n",
    "    #def fit(self, X, y, epochs, eta = 0.01,batchsize=64) :\n",
    "        # sauvegarde historique coût et accuracy pour affichage\n",
    "        cost_history = []\n",
    "        accuracy_history = []\n",
    "        parameter_history = []\n",
    "        for i in range(epochs):\n",
    "            i+=1\n",
    "            # sauvegarde des coûts et accuracy par mini-batch\n",
    "            cost_batch = []\n",
    "            accuracy_batch = []\n",
    "            # Descente de gradient par mini-batch\n",
    "            for (batchX, batchy) in self.next_batch(X, y, batchsize):\n",
    "                # Extraction et traitement d'un batch à la fois\n",
    "                \n",
    "                # mise en place des données au bon format\n",
    "                batchX=np.transpose(batchX)\n",
    "                if self.layers[self.nbLayers].activation_func==softmax:\n",
    "                    # la classification n'est pas binaire, y a utilisé one-hot-encoder\n",
    "                    # le batchy doit donc être transposé et le résultat doit\n",
    "                    # être sous la forme d'une matrice de taille batchy.shape[1]\n",
    "                    \n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], batchy.shape[1])))\n",
    "                else:\n",
    "                    # il s'agit d'une classification binaire donc shape[1] n'existe\n",
    "                    # pas\n",
    "                    batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                #batchy=np.transpose(batchy.reshape((batchy.shape[0], 1)))\n",
    "                self.forward_propagation(batchX)\n",
    "                #self.backward_propagation(batchy)\n",
    "                #self.update_parameters(eta)\n",
    "                \n",
    "                # sauvegarde pour affichage\n",
    "                current_cost=self.cost_function(batchy)\n",
    "                cost_batch.append(current_cost)\n",
    "                y_hat = self.predict(batchX)\n",
    "                current_accuracy = self.accuracy(y_hat, batchy)\n",
    "                accuracy_batch.append(current_accuracy)\n",
    "               \n",
    "            # SaveStats on W, B as well as values for A,Z, W, b\n",
    "            save_values = {}\n",
    "            save_values[\"epoch\"]=i\n",
    "            for l in range(1, self.nbLayers+1):\n",
    "                save_values[\"layer\"+str(l)]=l\n",
    "                save_values[\"Wmean\"+ str(l)]=np.mean(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmax\"+ str(l)]=np.amax(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wmin\"+str(l)]=np.amin(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"Wstd\"+str(l)]=np.std(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values[\"bmean\"+ str(l)]=np.mean(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmax\"+ str(l)]=np.amax(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bmin\"+str(l)]=np.amin(self.layers[self.nbLayers].parameters['b'])\n",
    "                save_values[\"bstd\"+str(l)]=np.std(self.layers[self.nbLayers].parameters['b'])\n",
    "                # be careful A,Z,W and b must be copied otherwise it is a referencee\n",
    "                save_values['A'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['A'])\n",
    "                save_values['Z'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['Z'])\n",
    "                save_values['W'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['W'])\n",
    "                save_values['b'+str(l)]=np.copy(self.layers[self.nbLayers].parameters['b'])\n",
    "                \n",
    "            parameter_history.append(save_values)        \n",
    "            # sauvegarde de la valeur moyenne des coûts et de l'accuracy du batch pour affichage\n",
    "            current_cost=np.average(cost_batch)\n",
    "            cost_history.append(current_cost)\n",
    "            current_accuracy=np.average(accuracy_batch)\n",
    "            accuracy_history.append(current_accuracy)\n",
    "        \n",
    "            if(verbose == True):\n",
    "                print(\"Epoch : #%s/%s - %s/%s - cost : %.4f - accuracy : %.4f\"%(i,epochs,X.shape[1],X.shape[1], float(current_cost), current_accuracy))\n",
    "              \n",
    "        return self.layers, cost_history, accuracy_history, parameter_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h1> Applications </h1></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of the network :\n",
      "\n",
      "\tLayer n° 0 du CNN => \n",
      "\t\tInput : None\n",
      "\t\tOutput : 4\n",
      "\n",
      "\tLayer n° 1 du CNN => \n",
      "\t\tInput : 4\n",
      "\t\tOutput : 10\n",
      "\t\tCouche : <bound method Conv3x3.forward of <__main__.Conv3x3 object at 0x000001E12CFB5388>>\n",
      "\t\tW shape : (10, 4)\n",
      "\t\tW data :\n",
      "[[-0.89382021  1.08039224 -0.68639629  0.3327359 ]\n",
      " [-0.07120333  0.21481422 -1.22043974  1.12083168]\n",
      " [ 0.09496203 -0.78266501  1.11597399  0.07601252]\n",
      " [-0.5402634  -0.54814105  0.97852771  0.53767345]\n",
      " [-0.20198188  0.38068329 -1.47353744  0.66311181]\n",
      " [-0.00408566 -0.33055755 -0.29911699  0.75270251]\n",
      " [-2.05299092  1.14331415  1.00798771 -0.47005285]\n",
      " [ 0.69651281 -1.20241052 -0.77791915 -0.93030086]\n",
      " [ 0.57311069 -0.73876987 -0.48735876 -0.60299016]\n",
      " [ 0.78734368 -1.35847214 -0.49596641  0.60039578]]\n",
      "\t\tb shape : (10, 1)\n",
      "\t\tb data :\n",
      "[[ 0.10506998]\n",
      " [-0.05368745]\n",
      " [-0.00291927]\n",
      " [ 0.1394056 ]\n",
      " [-0.03978674]\n",
      " [ 0.10191274]\n",
      " [ 0.18988813]\n",
      " [-0.03548496]\n",
      " [-0.03870165]\n",
      " [-0.19273054]]\n",
      "\n",
      "\tLayer n° 2 du CNN => \n",
      "\t\tInput : 10\n",
      "\t\tOutput : 3\n",
      "\t\tCouche : <bound method MaxPool2.forward of <__main__.MaxPool2 object at 0x000001E12CFB5708>>\n",
      "\t\tW shape : (3, 10)\n",
      "\t\tW data :\n",
      "[[-0.56530154  0.68330005 -0.43411513  0.21044066 -0.04503294  0.13586044\n",
      "  -0.77187386  0.7088762   0.06005926 -0.49500081]\n",
      " [ 0.70580392  0.04807454 -0.34169258 -0.34667484  0.61887527  0.34005455\n",
      "  -0.12774456  0.24076525 -0.93194691  0.41938873]\n",
      " [-0.002584   -0.20906295 -0.18917819  0.47605086 -1.29842546  0.72309536\n",
      "   0.6375074  -0.29728753  0.44051338 -0.76047119]]\n",
      "\t\tb shape : (3, 1)\n",
      "\t\tb data :\n",
      "[[-0.11001438]\n",
      " [-0.13156441]\n",
      " [ 0.08105009]]\n",
      "\n",
      "\tLayer n° 3 du CNN => \n",
      "\t\tInput : 3\n",
      "\t\tOutput : 2\n",
      "\t\tCouche : <bound method Flatten.forward of <__main__.Flatten object at 0x000001E12CFC0888>>\n",
      "\t\tW shape : (2, 3)\n",
      "\t\tW data :\n",
      "[[-1.03209468  1.2475295  -0.79258216]\n",
      " [ 0.38421032 -0.08221852  0.2480461 ]]\n",
      "\t\tb shape : (2, 1)\n",
      "\t\tb data :\n",
      "[[-0.17259624]\n",
      " [ 0.15850954]]\n",
      "\n",
      "\tLayer n° 0 du NN => \n",
      "\t\tInput : 2\n",
      "\t\tOutput : 10\n",
      "\t\tCouche : <function relu at 0x000001E12CFA8CA8>\n",
      "\t\tW shape : (10, 2)\n",
      "\t\tW data :\n",
      "[[-1.26405266  1.52790535]\n",
      " [-0.97071094  0.47055962]\n",
      " [-0.10069672  0.30379318]\n",
      " [-1.72596243  1.58509537]\n",
      " [ 0.13429659 -1.10685547]\n",
      " [ 1.57822555  0.10749794]\n",
      " [-0.76404783 -0.77518851]\n",
      " [ 1.38384717  0.76038508]\n",
      " [-0.28564551  0.53836748]\n",
      " [-2.08389663  0.93778171]]\n",
      "\t\tb shape : (10, 1)\n",
      "\t\tb data :\n",
      "[[-0.0005778 ]\n",
      " [-0.0467479 ]\n",
      " [-0.04230153]\n",
      " [ 0.10644821]\n",
      " [-0.29033676]\n",
      " [ 0.16168904]\n",
      " [ 0.14255099]\n",
      " [-0.06647551]\n",
      " [ 0.09850179]\n",
      " [-0.17004653]]\n",
      "\n",
      "\tLayer n° 1 du NN => \n",
      "\t\tInput : 10\n",
      "\t\tOutput : 5\n",
      "\t\tCouche : <function softmax at 0x000001E12CFDA168>\n",
      "\t\tW shape : (5, 10)\n",
      "\t\tW data :\n",
      "[[-0.56530154  0.68330005 -0.43411513  0.21044066 -0.04503294  0.13586044\n",
      "  -0.77187386  0.7088762   0.06005926 -0.49500081]\n",
      " [ 0.70580392  0.04807454 -0.34169258 -0.34667484  0.61887527  0.34005455\n",
      "  -0.12774456  0.24076525 -0.93194691  0.41938873]\n",
      " [-0.002584   -0.20906295 -0.18917819  0.47605086 -1.29842546  0.72309536\n",
      "   0.6375074  -0.29728753  0.44051338 -0.76047119]\n",
      " [-0.49199927 -0.58837392  0.36246703 -0.46723909 -0.30823275 -0.38136446\n",
      "   0.49795986 -0.85917322 -0.3136767   0.37972363]\n",
      " [ 0.46988723 -0.24009756 -0.01305538  0.62344079 -0.17793173  0.45576762\n",
      "   0.84920553 -0.15869359 -0.17307903 -0.86191717]]\n",
      "\t\tb shape : (5, 1)\n",
      "\t\tb data :\n",
      "[[ 0.05507566]\n",
      " [ 0.09543558]\n",
      " [ 0.07827765]\n",
      " [-0.17932379]\n",
      " [ 0.12418555]]\n"
     ]
    }
   ],
   "source": [
    "myNetwork = MyNeuralNetwork()\n",
    "\n",
    "myNetwork.addLayer(MyLayer(type = \"CNN\", input = 4, output = 10, couche = \"Conv3x3\", paramCouche = \"8\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"CNN\", output = 3, couche = \"MaxPool2\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"CNN\", output = 2, couche = \"flatten\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"RNN\", output = 10, activation = \"relu\"))\n",
    "myNetwork.addLayer(MyLayer(type = \"RNN\", output = 5, activation = \"softmax\"))\n",
    "\n",
    "myNetwork.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : #1/100 - 6/6 - cost : 0.1113 - accuracy : 1.0000\n",
      "Epoch : #2/100 - 6/6 - cost : 0.1069 - accuracy : 1.0000\n",
      "Epoch : #3/100 - 6/6 - cost : 0.1029 - accuracy : 1.0000\n",
      "Epoch : #4/100 - 6/6 - cost : 0.0989 - accuracy : 1.0000\n",
      "Epoch : #5/100 - 6/6 - cost : 0.0951 - accuracy : 1.0000\n",
      "Epoch : #6/100 - 6/6 - cost : 0.0916 - accuracy : 1.0000\n",
      "Epoch : #7/100 - 6/6 - cost : 0.0882 - accuracy : 1.0000\n",
      "Epoch : #8/100 - 6/6 - cost : 0.0849 - accuracy : 1.0000\n",
      "Epoch : #9/100 - 6/6 - cost : 0.0819 - accuracy : 1.0000\n",
      "Epoch : #10/100 - 6/6 - cost : 0.0790 - accuracy : 1.0000\n",
      "Epoch : #11/100 - 6/6 - cost : 0.0763 - accuracy : 1.0000\n",
      "Epoch : #12/100 - 6/6 - cost : 0.0737 - accuracy : 1.0000\n",
      "Epoch : #13/100 - 6/6 - cost : 0.0712 - accuracy : 1.0000\n",
      "Epoch : #14/100 - 6/6 - cost : 0.0688 - accuracy : 1.0000\n",
      "Epoch : #15/100 - 6/6 - cost : 0.0665 - accuracy : 1.0000\n",
      "Epoch : #16/100 - 6/6 - cost : 0.0644 - accuracy : 1.0000\n",
      "Epoch : #17/100 - 6/6 - cost : 0.0623 - accuracy : 1.0000\n",
      "Epoch : #18/100 - 6/6 - cost : 0.0603 - accuracy : 1.0000\n",
      "Epoch : #19/100 - 6/6 - cost : 0.0584 - accuracy : 1.0000\n",
      "Epoch : #20/100 - 6/6 - cost : 0.0566 - accuracy : 1.0000\n",
      "Epoch : #21/100 - 6/6 - cost : 0.0549 - accuracy : 1.0000\n",
      "Epoch : #22/100 - 6/6 - cost : 0.0533 - accuracy : 1.0000\n",
      "Epoch : #23/100 - 6/6 - cost : 0.0517 - accuracy : 1.0000\n",
      "Epoch : #24/100 - 6/6 - cost : 0.0502 - accuracy : 1.0000\n",
      "Epoch : #25/100 - 6/6 - cost : 0.0487 - accuracy : 1.0000\n",
      "Epoch : #26/100 - 6/6 - cost : 0.0473 - accuracy : 1.0000\n",
      "Epoch : #27/100 - 6/6 - cost : 0.0460 - accuracy : 1.0000\n",
      "Epoch : #28/100 - 6/6 - cost : 0.0447 - accuracy : 1.0000\n",
      "Epoch : #29/100 - 6/6 - cost : 0.0435 - accuracy : 1.0000\n",
      "Epoch : #30/100 - 6/6 - cost : 0.0424 - accuracy : 1.0000\n",
      "Epoch : #31/100 - 6/6 - cost : 0.0413 - accuracy : 1.0000\n",
      "Epoch : #32/100 - 6/6 - cost : 0.0402 - accuracy : 1.0000\n",
      "Epoch : #33/100 - 6/6 - cost : 0.0392 - accuracy : 1.0000\n",
      "Epoch : #34/100 - 6/6 - cost : 0.0383 - accuracy : 1.0000\n",
      "Epoch : #35/100 - 6/6 - cost : 0.0373 - accuracy : 1.0000\n",
      "Epoch : #36/100 - 6/6 - cost : 0.0364 - accuracy : 1.0000\n",
      "Epoch : #37/100 - 6/6 - cost : 0.0356 - accuracy : 1.0000\n",
      "Epoch : #38/100 - 6/6 - cost : 0.0348 - accuracy : 1.0000\n",
      "Epoch : #39/100 - 6/6 - cost : 0.0340 - accuracy : 1.0000\n",
      "Epoch : #40/100 - 6/6 - cost : 0.0332 - accuracy : 1.0000\n",
      "Epoch : #41/100 - 6/6 - cost : 0.0325 - accuracy : 1.0000\n",
      "Epoch : #42/100 - 6/6 - cost : 0.0318 - accuracy : 1.0000\n",
      "Epoch : #43/100 - 6/6 - cost : 0.0311 - accuracy : 1.0000\n",
      "Epoch : #44/100 - 6/6 - cost : 0.0304 - accuracy : 1.0000\n",
      "Epoch : #45/100 - 6/6 - cost : 0.0298 - accuracy : 1.0000\n",
      "Epoch : #46/100 - 6/6 - cost : 0.0292 - accuracy : 1.0000\n",
      "Epoch : #47/100 - 6/6 - cost : 0.0286 - accuracy : 1.0000\n",
      "Epoch : #48/100 - 6/6 - cost : 0.0280 - accuracy : 1.0000\n",
      "Epoch : #49/100 - 6/6 - cost : 0.0274 - accuracy : 1.0000\n",
      "Epoch : #50/100 - 6/6 - cost : 0.0269 - accuracy : 1.0000\n",
      "Epoch : #51/100 - 6/6 - cost : 0.0264 - accuracy : 1.0000\n",
      "Epoch : #52/100 - 6/6 - cost : 0.0259 - accuracy : 1.0000\n",
      "Epoch : #53/100 - 6/6 - cost : 0.0254 - accuracy : 1.0000\n",
      "Epoch : #54/100 - 6/6 - cost : 0.0249 - accuracy : 1.0000\n",
      "Epoch : #55/100 - 6/6 - cost : 0.0244 - accuracy : 1.0000\n",
      "Epoch : #56/100 - 6/6 - cost : 0.0240 - accuracy : 1.0000\n",
      "Epoch : #57/100 - 6/6 - cost : 0.0236 - accuracy : 1.0000\n",
      "Epoch : #58/100 - 6/6 - cost : 0.0231 - accuracy : 1.0000\n",
      "Epoch : #59/100 - 6/6 - cost : 0.0227 - accuracy : 1.0000\n",
      "Epoch : #60/100 - 6/6 - cost : 0.0223 - accuracy : 1.0000\n",
      "Epoch : #61/100 - 6/6 - cost : 0.0219 - accuracy : 1.0000\n",
      "Epoch : #62/100 - 6/6 - cost : 0.0216 - accuracy : 1.0000\n",
      "Epoch : #63/100 - 6/6 - cost : 0.0212 - accuracy : 1.0000\n",
      "Epoch : #64/100 - 6/6 - cost : 0.0208 - accuracy : 1.0000\n",
      "Epoch : #65/100 - 6/6 - cost : 0.0205 - accuracy : 1.0000\n",
      "Epoch : #66/100 - 6/6 - cost : 0.0201 - accuracy : 1.0000\n",
      "Epoch : #67/100 - 6/6 - cost : 0.0198 - accuracy : 1.0000\n",
      "Epoch : #68/100 - 6/6 - cost : 0.0195 - accuracy : 1.0000\n",
      "Epoch : #69/100 - 6/6 - cost : 0.0192 - accuracy : 1.0000\n",
      "Epoch : #70/100 - 6/6 - cost : 0.0189 - accuracy : 1.0000\n",
      "Epoch : #71/100 - 6/6 - cost : 0.0186 - accuracy : 1.0000\n",
      "Epoch : #72/100 - 6/6 - cost : 0.0183 - accuracy : 1.0000\n",
      "Epoch : #73/100 - 6/6 - cost : 0.0180 - accuracy : 1.0000\n",
      "Epoch : #74/100 - 6/6 - cost : 0.0178 - accuracy : 1.0000\n",
      "Epoch : #75/100 - 6/6 - cost : 0.0175 - accuracy : 1.0000\n",
      "Epoch : #76/100 - 6/6 - cost : 0.0172 - accuracy : 1.0000\n",
      "Epoch : #77/100 - 6/6 - cost : 0.0170 - accuracy : 1.0000\n",
      "Epoch : #78/100 - 6/6 - cost : 0.0167 - accuracy : 1.0000\n",
      "Epoch : #79/100 - 6/6 - cost : 0.0165 - accuracy : 1.0000\n",
      "Epoch : #80/100 - 6/6 - cost : 0.0163 - accuracy : 1.0000\n",
      "Epoch : #81/100 - 6/6 - cost : 0.0160 - accuracy : 1.0000\n",
      "Epoch : #82/100 - 6/6 - cost : 0.0158 - accuracy : 1.0000\n",
      "Epoch : #83/100 - 6/6 - cost : 0.0156 - accuracy : 1.0000\n",
      "Epoch : #84/100 - 6/6 - cost : 0.0154 - accuracy : 1.0000\n",
      "Epoch : #85/100 - 6/6 - cost : 0.0152 - accuracy : 1.0000\n",
      "Epoch : #86/100 - 6/6 - cost : 0.0150 - accuracy : 1.0000\n",
      "Epoch : #87/100 - 6/6 - cost : 0.0148 - accuracy : 1.0000\n",
      "Epoch : #88/100 - 6/6 - cost : 0.0146 - accuracy : 1.0000\n",
      "Epoch : #89/100 - 6/6 - cost : 0.0144 - accuracy : 1.0000\n",
      "Epoch : #90/100 - 6/6 - cost : 0.0142 - accuracy : 1.0000\n",
      "Epoch : #91/100 - 6/6 - cost : 0.0140 - accuracy : 1.0000\n",
      "Epoch : #92/100 - 6/6 - cost : 0.0138 - accuracy : 1.0000\n",
      "Epoch : #93/100 - 6/6 - cost : 0.0137 - accuracy : 1.0000\n",
      "Epoch : #94/100 - 6/6 - cost : 0.0135 - accuracy : 1.0000\n",
      "Epoch : #95/100 - 6/6 - cost : 0.0133 - accuracy : 1.0000\n",
      "Epoch : #96/100 - 6/6 - cost : 0.0132 - accuracy : 1.0000\n",
      "Epoch : #97/100 - 6/6 - cost : 0.0130 - accuracy : 1.0000\n",
      "Epoch : #98/100 - 6/6 - cost : 0.0128 - accuracy : 1.0000\n",
      "Epoch : #99/100 - 6/6 - cost : 0.0127 - accuracy : 1.0000\n",
      "Epoch : #100/100 - 6/6 - cost : 0.0125 - accuracy : 1.0000\n",
      "Accuracy test: 0.000\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "eta = 0.01\n",
    "batchsize=32\n",
    "\n",
    "#Entraînement du classifieur\n",
    "layers,cost_history,accuracy_history,parameter_history=network.fit(X_train, y_train, verbose=True, epochs=epochs)\n",
    "\n",
    "\n",
    "#Prédiction\n",
    "y_pred=network.predict(X_test)\n",
    "accuracy_test = network.accuracy(y_pred, y_test)\n",
    "print(\"Accuracy test: %.3f\"%accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
